[
  {
    "objectID": "notebooks/01c-scientific_computing.html",
    "href": "notebooks/01c-scientific_computing.html",
    "title": "Dask Delayed versus Multiprocessing",
    "section": "",
    "text": "import dask\nimport dask.distributed as dd\nimport dask.array as da\nimport dask.dataframe as dd\nimport pandas as pd\nimport random\nfrom multiprocessing import Pool ### The default pool makes one process per CPU\n\nSpeed up a function that could take a while to run\n\n# reference:\n# https://aaltoscicomp.github.io/python-for-scicomp/parallel/\n\ndef sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n# Using apply from pandas\nps = pd.Series([10**5,20**5])\nps.apply(sample)\n0    3.13884\n1    3.14272\ndtype: float64\n# Create a pool object from with a with statement \nwith Pool() as p:\n    result = p.map(sample,ps)\n    # will engage p.close() automatically\nProcess SpawnPoolWorker-1:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-2:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-10:\nProcess SpawnPoolWorker-8:\nProcess SpawnPoolWorker-11:\nProcess SpawnPoolWorker-9:\nProcess SpawnPoolWorker-3:\nProcess SpawnPoolWorker-7:\nProcess SpawnPoolWorker-4:\nProcess SpawnPoolWorker-12:\nProcess SpawnPoolWorker-6:\nProcess SpawnPoolWorker-5:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In [4], line 3\n      1 # Create a pool object from with a with statement \n      2 with Pool() as p:\n----&gt; 3     result = p.map(sample,ps)\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:364, in Pool.map(self, func, iterable, chunksize)\n    359 def map(self, func, iterable, chunksize=None):\n    360     '''\n    361     Apply `func` to each element in `iterable`, collecting the results\n    362     in a list that is returned.\n    363     '''\n--&gt; 364     return self._map_async(func, iterable, mapstar, chunksize).get()\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:765, in ApplyResult.get(self, timeout)\n    764 def get(self, timeout=None):\n--&gt; 765     self.wait(timeout)\n    766     if not self.ready():\n    767         raise TimeoutError\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:762, in ApplyResult.wait(self, timeout)\n    761 def wait(self, timeout=None):\n--&gt; 762     self._event.wait(timeout)\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/threading.py:581, in Event.wait(self, timeout)\n    579 signaled = self._flag\n    580 if not signaled:\n--&gt; 581     signaled = self._cond.wait(timeout)\n    582 return signaled\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/threading.py:312, in Condition.wait(self, timeout)\n    310 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    311     if timeout is None:\n--&gt; 312         waiter.acquire()\n    313         gotit = True\n    314     else:\n\nKeyboardInterrupt: \n\nMultiprocessing introduces an initial fixed cost in time (creating Pool objects). Knowing what hardware you are working on is needed to tailor the number of processes created with what is available. There is a risk of creating too many processes which make the initial fixed cost excessively large.\n\n\nWe will come back to the below alternative (Dask) afterwards\n# Create the dask equivalent input\nds = dd.from_pandas(ps,npartitions = 2)\n%%timeit #605 ms ± 10.9 ms per loop\nresult = ds.apply(sample,meta=('x', 'float64')).mean().compute()\n894 ms ± 9.68 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n%%timeit # 1.08 s ± 47.8 ms per loop\np = Pool()\nresult = p.map(sample,ps)\np.close()\nProcess SpawnPoolWorker-13:\nProcess SpawnPoolWorker-14:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-20:\nProcess SpawnPoolWorker-24:\nProcess SpawnPoolWorker-21:\nProcess SpawnPoolWorker-23:\nProcess SpawnPoolWorker-16:\nProcess SpawnPoolWorker-22:\nProcess SpawnPoolWorker-17:\nTraceback (most recent call last):\nProcess SpawnPoolWorker-19:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nProcess SpawnPoolWorker-15:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nProcess SpawnPoolWorker-18:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In [7], line 1\n----&gt; 1 get_ipython().run_cell_magic('timeit', '# 1.08 s ± 47.8 ms per loop', 'p = Pool()\\nresult = p.map(sample,ps)\\np.close()\\n')\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2362, in InteractiveShell.run_cell_magic(self, magic_name, line, cell)\n   2360 with self.builtin_trap:\n   2361     args = (magic_arg_s, cell)\n-&gt; 2362     result = fn(*args, **kwargs)\n   2363 return result\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/IPython/core/magics/execution.py:1162, in ExecutionMagics.timeit(self, line, cell, local_ns)\n   1160 for index in range(0, 10):\n   1161     number = 10 ** index\n-&gt; 1162     time_number = timer.timeit(number)\n   1163     if time_number &gt;= 0.2:\n   1164         break\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/IPython/core/magics/execution.py:156, in Timer.timeit(self, number)\n    154 gc.disable()\n    155 try:\n--&gt; 156     timing = self.inner(it, self.timer)\n    157 finally:\n    158     if gcold:\n\nFile &lt;magic-timeit&gt;:2, in inner(_it, _timer)\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:364, in Pool.map(self, func, iterable, chunksize)\n    359 def map(self, func, iterable, chunksize=None):\n    360     '''\n    361     Apply `func` to each element in `iterable`, collecting the results\n    362     in a list that is returned.\n    363     '''\n--&gt; 364     return self._map_async(func, iterable, mapstar, chunksize).get()\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:765, in ApplyResult.get(self, timeout)\n    764 def get(self, timeout=None):\n--&gt; 765     self.wait(timeout)\n    766     if not self.ready():\n    767         raise TimeoutError\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py:762, in ApplyResult.wait(self, timeout)\n    761 def wait(self, timeout=None):\n--&gt; 762     self._event.wait(timeout)\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/threading.py:581, in Event.wait(self, timeout)\n    579 signaled = self._flag\n    580 if not signaled:\n--&gt; 581     signaled = self._cond.wait(timeout)\n    582 return signaled\n\nFile ~/opt/miniconda3/envs/quantum/lib/python3.9/threading.py:312, in Condition.wait(self, timeout)\n    310 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    311     if timeout is None:\n--&gt; 312         waiter.acquire()\n    313         gotit = True\n    314     else:\n\nKeyboardInterrupt: \n\n\n\nCompare Multiprocessing to Dask\nDask uses multiprocessing by default to overcome the GIL. Hence comparing the run time of the multiprocessing library to Dask with a function-bound problem will yield similar results.\nYet dask offers an ecosystem of resource management (Scheduler, diagnostics, data partitions and Task Graphs) that make it a more attractive way to achieve the same thing in most cases. Resource management is handled automatically by the scheduler.\n# for reference, delaying the same function.\n@dask.delayed\ndef dd_sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n\n\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\nresult\n3.1386374999999997\n%%timeit #595 ms ± 1.54 ms per loop\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\n855 ms ± 7.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nKey points\n\nUsing Multiprocessing (or mpi4py - not covered here) are the traditional ways to make functions run in parallel in Python\nUsing Dask and its ecosystem is the modern approach"
  },
  {
    "objectID": "notebooks/polars-intro.html",
    "href": "notebooks/polars-intro.html",
    "title": "Polars",
    "section": "",
    "text": "Polars is a lightning fast DataFrame library. The key features of polars are:\nFast and Accessible: Written from scratch in Rust, designed close to the machine and without external dependencies. It also has python and R bindings!\nI/O: First class support for all common data storage layers: local, cloud storage & databases.\nHandle Datasets larger than RAM\nIntuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.\nOut of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time\nParallel: Utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration.\nThe philosophy of Polars is to provide a dataframe library that utilises available cores, has an intuitive api and is performant - hence adheres to a strict schema (data-types should be known before running the query)."
  },
  {
    "objectID": "notebooks/polars-intro.html#getting-to-know-the-syntax",
    "href": "notebooks/polars-intro.html#getting-to-know-the-syntax",
    "title": "Polars",
    "section": "Getting to know the syntax",
    "text": "Getting to know the syntax\nWe will cover some basic syntax. Check out the API documentation for more information.\n\nReading and Writing Data\nPolars support reading and writing data many types of files. It also supporting reading from a database, or cloud storage. See the IO Documentation for more information.\n# writing data to disk\ndf.write_csv(\"output.csv\")\ndf.write_parquet(\"output.parquet\")\n# reading back into a polars dataframe\nnew_df = pl.read_csv(\"output.csv\")\nnew_df = pl.read_parquet(\"output.parquet\")\n\n# can convert back to pandas dataframe (or other types)\ndf.to_pandas() \n\n\n\n\n\n\n\ninteger\ndate\nfloat\nstring\n\n\n\n\n0\n1\n2025-01-01\n4.0\na\n\n\n1\n2\n2025-01-02\n5.0\nb\n\n\n2\n3\n2025-01-03\n6.0\nc\n\n\n3\n4\n2025-01-03\n12.0\nb\n\n\n\n\n\n# selecting columns ----------------------------------------------------------------\n\ndf.select(pl.col(\"float\")) # selecting a column\n\ndf.select(pl.col(\"date\",\"string\")) # selecting multiple columns \n\ndf.select(pl.col(\"*\").exclude(\"string\")) # select all columns then exclude\n\ndf.select(pl.col(\"^(da|fl).*$\")) # supports regular expressions\n\n# selectors are intuitive helper funtions for selecting columns by name or type\nimport polars.selectors as cs   \n\ndf.select(cs.integer() | cs.contains(\"ate\")) # select all columns that are integers or contains ate\n\n# filtering rows ---------------------------------------------------------------------\n\ndf.filter(pl.col(\"integer\") &gt;= 2)  #filtering rows\n\ndf.filter((pl.col(\"integer\") &gt;=2) & \n          (pl.col(\"float\") == 5.0)) #filtering rows with multiple conditions (| = or & = and)\n\n#creating / manipulating columns -----------------------------------------------------\n\ndf.with_columns((pl.col(\"integer\") + 3).alias(\"new_column\")) # creating column and naming it\n\n#group by aggregations ----------------------------------------------------------------\n\ndf.group_by(\"string\").agg(pl.col(\"integer\").sum().alias(\"sum\"),\n                          pl.col(\"date\").sort().first().alias(\"earliest\"), \n                          pl.col(\"float\") / pl.col(\"integer\")) \n                                \n\nshape: (3, 4)\n\n\n\nstring\nsum\nearliest\nfloat\n\n\nstr\ni64\ndatetime[μs]\nlist[f64]\n\n\n\n\n\"b\"\n6\n2025-01-02 00:00:00\n[2.5, 3.0]\n\n\n\"a\"\n1\n2025-01-01 00:00:00\n[4.0]\n\n\n\"c\"\n3\n2025-01-03 00:00:00\n[2.0]\n\n\n\n\n# Can combine expressions for compactness\n\ndf3 = df.with_columns((pl.col(\"float\") * pl.col(\"integer\"))\n                .alias(\"product\")).select(pl.all().exclude(\"integer\"))\n\n\nLinks to documentation:\nSelecting columns\nAggregation\nData Transformation such as join, Concatenation, pivot and unpivot.\nFunctions for expressions.\n\n\nData types and casting\nMost data types are specified by the arrow syntax with the exception of String, Categorical and Object types.\nCategorical data represents string data where the values in the column have a finite set of values (yet for performance implementation different to strings). Polars supports both Enum data type, where categories are known up front, and the more flexible Categorical data type where values are not known beforehand. Conversion between them is trivial. Relying on polars inferring the categories with Categorical types comes at a performance cost. See Categorical page for more information.\nCasting (changing the datatypes) is enabled by either specifying the dtype argument or applying the cast() function.\n# Use Enum where categories are known\ncat_types = pl.Enum([\"polar\",\"panda\",\"teddy\"])\nanimals = pl.Series([\"polar\",\"polar\",\"teddy\",\"panda\"],dtype= cat_types)\n# Use Categprical otherwise\nfictional_animals = pl.Series([\"poobear\",\"minimouse\",\"teddy\",\"poobear\"],dtype= pl.Categorical)\n\n# casting columns to other data types with cast\ndf.cast({\"integer\": pl.Float32, \"float\": pl.UInt8})\n\nshape: (4, 4)\n\n\n\ninteger\ndate\nfloat\nstring\n\n\nf32\ndatetime[μs]\nu8\nstr\n\n\n\n\n1.0\n2025-01-01 00:00:00\n4\n\"a\"\n\n\n2.0\n2025-01-02 00:00:00\n5\n\"b\"\n\n\n3.0\n2025-01-03 00:00:00\n6\n\"c\"\n\n\n4.0\n2025-01-03 00:00:00\n12\n\"b\""
  },
  {
    "objectID": "notebooks/polars-intro.html#lazy-eager-and-streaming",
    "href": "notebooks/polars-intro.html#lazy-eager-and-streaming",
    "title": "Polars",
    "section": "Lazy / Eager and Streaming",
    "text": "Lazy / Eager and Streaming\nPolars supports two modes of operation: lazy and eager. In the eager API the query is executed immediately while in the lazy API the query is only evaluated once it is ‘needed’. Deferring the execution to the last minute can have significant performance advantages.\nAn example of using the eager API is below. Every step is executed immediately returning the intermediate results. This can be very wasteful as we might do work or load extra data that is not being used.\ndf = pl.read_csv(\"iris_data.csv\") #read the iris dataset\ndf_small = df.filter(pl.col(\"sepal length\") &gt; 5) #filter\ndf_agg = df_small.group_by(\"species\").agg(pl.col(\"sepal width\").mean()) #mean of the sepal width per species\nIf we instead used the lazy API and waited on execution until all the steps are defined then the query planner could perform various optimizations.\nq = (\n    pl.scan_csv(\"iris_data.csv\") #doesnt read it all before other operation is performed\n        .filter(pl.col(\"sepal length\") &gt; 5)\n        .group_by(\"species\").agg(pl.col(\"sepal width\").mean())\n)\n\nq # a lazyframe\n\ndf_agg = q.collect() # inform polars that you want to execute the query\ndf_agg = q.collect(streaming=True) # with streaming mode to process in batches\nStreaming\nOne additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once Polars can execute the query in batches allowing you to process datasets that are larger-than-memory. See here for more info on streaming.\nWhen to use Lazy versus Eager:\nIn general the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don’t know yet what your query is going to look like\nWhen using Lazy mode, apply filteres as early as possible before reading the data. Only select column you need.\n\nCommon Machine Learning Workflow\nGiven your new knowledge of polars, here is an example on how to integrate into the usual pipeline consisting of data ingestion and manipulation, model preperation and prediction.\n# loading libraries\n!pip install ucimlrepo\n!pip install scikit-learn==1.4\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import plot_tree\nfrom matplotlib import pylab as plt\n# Fit a descision tree from polars dataframe object\ndf = pl.read_csv(\"iris_data.csv\")\ndata = df.select(['sepal length', 'sepal width', 'petal length', 'petal width'])\ntarget = df.select(['species']).to_series()\nclf = DecisionTreeClassifier()\nmodel = clf.fit(data, target)\n# Plot the decision tree\nplot_tree(clf, filled=True, feature_names=data.columns)\nplt.title(\"Decision tree trained on all the iris features\")\nplt.show()\n\n# Add predicitions to the polars dataframe\npredict_df = pl.concat([df,\n                        pl.DataFrame(model.predict(data), schema=[\"predict\"])],\n                       how=\"horizontal\")\nprint(predict_df.sample(8))\nshape: (8, 6)\n┌──────────────┬─────────────┬──────────────┬─────────────┬─────────────────┬─────────────────┐\n│ sepal length ┆ sepal width ┆ petal length ┆ petal width ┆ species ┆ predict │\n│ — ┆ — ┆ — ┆ — ┆ — ┆ — │\n│ f64 ┆ f64 ┆ f64 ┆ f64 ┆ str ┆ str │\n╞══════════════╪═════════════╪══════════════╪═════════════╪═════════════════╪═════════════════╡\n│ 6.0 ┆ 3.4 ┆ 4.5 ┆ 1.6 ┆ Iris-versicolor ┆ Iris-versicolor │\n│ 6.3 ┆ 2.8 ┆ 5.1 ┆ 1.5 ┆ Iris-virginica ┆ Iris-virginica │\n│ 5.7 ┆ 4.4 ┆ 1.5 ┆ 0.4 ┆ Iris-setosa ┆ Iris-setosa │\n│ 5.8 ┆ 2.7 ┆ 5.1 ┆ 1.9 ┆ Iris-virginica ┆ Iris-virginica │\n│ 6.6 ┆ 3.0 ┆ 4.4 ┆ 1.4 ┆ Iris-versicolor ┆ Iris-versicolor │\n│ 6.9 ┆ 3.1 ┆ 5.4 ┆ 2.1 ┆ Iris-virginica ┆ Iris-virginica │\n│ 6.7 ┆ 2.5 ┆ 5.8 ┆ 1.8 ┆ Iris-virginica ┆ Iris-virginica │\n│ 4.8 ┆ 3.0 ┆ 1.4 ┆ 0.3 ┆ Iris-setosa ┆ Iris-setosa │\n└──────────────┴─────────────┴──────────────┴─────────────┴─────────────────┴─────────────────┘"
  },
  {
    "objectID": "notebooks/polars-intro.html#ecosystem",
    "href": "notebooks/polars-intro.html#ecosystem",
    "title": "Polars",
    "section": "Ecosystem",
    "text": "Ecosystem\nOn the Supported Polars Ecosystem page you can find a non-exhaustive list of libraries and tools that support Polars.\nAs the data ecosystem is evolving fast, more libraries will likely support Polars in the future."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html",
    "href": "notebooks/01b-TransitionToDask.html",
    "title": "Dask",
    "section": "",
    "text": "Dask\n10 minute intro\nAPI Reference\nExamples\nDask is a Python library for parallel and distributed computing. Dask offers an array of tools such as Dask DataFrames, Dask Bags, Dask Delayed Functions and has machine learning integrations. We will foucus with Dask DataFrames.\nDask dataframes process data in parallel across multiple cores or machines.\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index.\nDask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\nInternally, a Dask DataFrame is split into many partitions, where each partition is one Pandas DataFrame. When our index is sorted and we know the values of the divisions of our partitions, then we can be clever and efficient with expensive algorithms (e.g. groupby’s, joins, etc…).\nUse Cases:\nDask DataFrame is used in situations where pandas is commonly needed, usually when pandas fails due to data size or speed of computation. Common use cases are:\nDask DataFrame may not be the best choice in the following situations:"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "title": "Dask",
    "section": "Transitioning to Dask DataFrames",
    "text": "Transitioning to Dask DataFrames\nDask mimics most but not all of the well known pandas API syntax.\n# IO ---------------------------------------------------------------- \n  \n# loading into a dask dataframe\nddf = dd.read_csv('diamonds.csv') #reading multiple files with patterns dd.read_csv('d*.csv')\nddf = dd.from_pandas(df,npartitions = 2) \n\n# by default it has lazy execution where computation are triggered by compute() (or head) \nddf.compute() # convert dd to pd.DataFrame\nddf.head(2)\n\n# Inspecting ----------------------------------------------------------------\nddf.npartitions # number of partitions\nddf.divisions # Divisions includes the minimum value of every partition’s index and the maximum value of the last partition’s index\nddf.partitions[1] # access a particular partition\nddf.partitions[1].index # which have similar pd.DataFrame attributes\nddf.describe().compute() # general stats\nddf.dtypes  # access attributes\n\n# Columns \nddf[[\"carat\",\"price\"]] # subset columns\nddf['new_column'] = ddf['carat'] * ddf['depth'] # creating new column\n\n# Filtering rows ----------------------------------------------------------------\n\nddf.query('price &gt; 50') # same as pd.DataFrame\n\nddf.loc[15:20] # subseing rows \n\n# Group by Aggregations ----------------------------------------------------------------\n\n# By default, groupby methods return an object with only 1 partition. \n# This is to optimize performance, and assumes the groupby reduction returns an object that is small enough to fit into memory. \n# If your returned object is larger than this, you can increase the number of output partitions using the split_out argument.\nddf.groupby('cut').price.mean() #npartitions=1\n#ddf.groupby('cut').mean(split_out=2) #npartitions=2\n\n# dask aggregate has more features than pandas agg equivalent, supports reductions on the same group.\nddf_aggs = (ddf.groupby('cut')\n    .aggregate({\"price\":\"mean\",\"carat\":\"sum\"})).compute()\nddf_aggs\n\n\n\n\n\n\n\nprice\ncarat\n\n\ncut\n\n\n\n\n\n\nIdeal\n3457.541970\n15146.84\n\n\nPremium\n4584.257704\n12300.95\n\n\nGood\n3928.864452\n4166.10\n\n\nVery Good\n3981.759891\n9742.70\n\n\nFair\n4358.757764\n1684.28"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#contolling-laziness-and-optimising-with-dask",
    "href": "notebooks/01b-TransitionToDask.html#contolling-laziness-and-optimising-with-dask",
    "title": "Dask",
    "section": "Contolling Laziness and Optimising with Dask",
    "text": "Contolling Laziness and Optimising with Dask\n\n#controlling when execution occurs is key to performance\nlazy_manipulations = (ddf.query('price &gt; 50').\n    groupby('clarity').\n    price.mean())\nlazy_manipulations.compute() # trigger computation to pd.DataFrame\n\n\n# Can persist data into RAM if possible making future operations on it faster\nddf_aggs = ddf_aggs.repartition(npartitions = 1).persist()\n\n\n\n\n\n\n\n\ncarat_original\ncut\ncolor\nclarity\ndepth\ntable\nprice_original\nx\ny\nz\nprice_aggregated\ncarat_aggregated\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n3457.54197\n15146.84\n\n\n11\n0.23\nIdeal\nJ\nVS1\n62.8\n56.0\n340\n3.93\n3.90\n2.46\n3457.54197\n15146.84\n\n\n\n\n\nNote that not all apis from pandas are available in Dask. For example, ddf.filter(['carat','price']) is not available. For more details and a list of available options, see here.\n\nChallenge\n\nWhat is the price per carat over the entire dataset?\nCreate a column called price_to_carat that calculates this for each row\nCreate a column called expensive that flags whether price is greater than price_to_carat\nHow many expensive diamonds are there\n\n\n\nSolution\n\n\nAverage price to carat $4928\n15003 expensive diamonds compared to whole dataset\n\nprice_per_carat = (ddf.price.sum() / ddf.carat.sum()).compute()\n\nddf = ddf.assign(price_to_carat = ddf.price / ddf.carat)\n\ndef greater_than_avg(price):\n    if price &gt; price_per_carat:\n        return True\n    else:\n        return False\n\nddf = ddf.assign(expensive = ddf.price.apply(greater_than_avg))\nddf.sort_values('expensive',ascending= False).compute()\nnumber_expensive = ddf.expensive.sum().compute()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "href": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "title": "Dask",
    "section": "Dask Best Practice Guide",
    "text": "Dask Best Practice Guide\n\nUse set_index() sparingly to speed up data naturally sorted on a single index\n\nUse ddf.set_index('column')\n\nPersist intelligently\n\n\nIf you have the available RAM for your dataset then you can persist data in memory. On distributed systems, it is a way of telling the cluster that it should start executing the computations that you have defined so far, and that it should try to keep those results in memory.\n\ndf = df.persist()\n\n\n\nRepartition to reduce overhead\n\nAs you reduce or increase the size of your pandas DataFrames by filtering or joining, it may be wise to reconsider how many partitions you need. Adjust partitions accordingly using repartition.\ndf = df.repartition(npartitions=df.npartitions // 100)\n\nConsider storing large data in Apache Parquet Format (binary column based format)"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#additional-info-on-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#additional-info-on-dask-dataframes",
    "title": "Dask",
    "section": "Additional Info on Dask DataFrames",
    "text": "Additional Info on Dask DataFrames\nTo learn more on how to use dask dataframes, feel free to go through Reading Messy Data Binder at your own pace.This example demostrates both dask and using delayed functions. Otherwise here is more info including best practices and the API, and other tutorials on dask is also available."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "href": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "title": "Dask",
    "section": "Dask Arrays",
    "text": "Dask Arrays\nimport dask.array as da\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nx\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n762.94 MiB\n7.63 MiB\n\n\nShape\n(10000, 10000)\n(1000, 1000)\n\n\nDask graph\n100 chunks in 1 graph layer\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10000\n10000\n\n\n\n\n# numpy syntax as usual\ny = x + x.T\nz = y[::2, 5000:].mean(axis=1) # axis 0 is index, axis 1 is columns\nz\n# Trigger compute and investigate Client\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n39.06 kiB\n3.91 kiB\n\n\nShape\n(5000,)\n(500,)\n\n\nCount\n7 Graph Layers\n10 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5000\n1"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "href": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "title": "Dask",
    "section": "Diagnostics - Profile resource efficiency in real time",
    "text": "Diagnostics - Profile resource efficiency in real time\nThe Dask Dashboard enables resource monitoring across RAM, CPU, workers, threads and tasks (functions), in real time as your code is running. It opens a user friendly dashboard allowing you to inspect what processes are running and what work (or bottlenecks) exist across your cores.\nSee here for documentation and videos.\n\nA few key definitions:\n\nBytes Stored and Bytes per Worker: Cluster memory and Memory per worker.\nTask Processing/CPU Utilization/Occupancy: Tasks being processed by each worker/ CPU Utilization per worker/ Expected runtime for all tasks currently on a worker.\nProgress: Progress of a set of tasks.\n\nThere are three different colors of workers in a task graph:\n\nBlue: Processing tasks.\nGreen: Saturated: It has enough work to stay busy.\nRed: Idle: Does not have enough work to stay busy.\nTask Stream: Individual task across threads.\n\nWhite colour represents deadtime.\n\n\n# To load diagnostic in web browser on local. Wont work on CoLab.\nfrom dask.distributed import Client\nclient = Client()\nclient #client.shutdown after use\n\n     \n    \n        Client\n        Client-282909c2-4e0d-11ef-8d95-d687d7367288\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        2c397460\n        \n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 5\n\n\nTotal threads: 10\nTotal memory: 32.00 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-07b1460e-b35e-4c3c-afc5-5a9b007d4875\n            \n\n\n\nComm: tcp://127.0.0.1:50573\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 10\n\n\nStarted: Just now\nTotal memory: 32.00 GiB\n\n\n\n\n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n\n\n\nComm: tcp://127.0.0.1:50592\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50597/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50576\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-jt_xmqss\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 1\n                \n                \n\n\n\nComm: tcp://127.0.0.1:50593\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50598/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50577\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-1c5y5lww\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 2\n                \n                \n\n\n\nComm: tcp://127.0.0.1:50594\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50599/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50578\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-3b1scof8\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 3\n                \n                \n\n\n\nComm: tcp://127.0.0.1:50595\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50604/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50579\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-rkqgplj_\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 4\n                \n                \n\n\n\nComm: tcp://127.0.0.1:50596\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50603/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50580\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-n26yax3w\n\n\n\n\n            \n            \n        \n        \n\n    \n\n\n        \n    \n\n            \n        \n\n    \n\n# Example of efficient resource utilisation\nimport dask.array as da\nx = da.random.random(size = (10_000,10_000,10), chunks= (1000,1000,5))\ny = da.random.random(size = (10_000,10_000,10), chunks= (1000,1000,5))\nz = (da.arcsin(x) + da.arcsin(y)).sum(axis = (1,2))\nz.compute()\narray([114088.23038208, 114412.76898299, 114062.70087907, ...,\n       114507.50869038, 114166.89260871, 113983.68356763])\n# Inefficient resource utilisation - dask introduces too much overhead for simple sizes np handles well\nx = da.random.random(size = (10_000_000),chunks = (1000,))\nx.sum().compute()\n/Users/kris/miniconda3/envs/parallel/lib/python3.9/site-packages/distributed/client.py:3357: UserWarning: Sending large graph of size 25.26 MiB.\nThis may cause some slowdown.\nConsider scattering data ahead of time and using futures.\n  warnings.warn(\n5000296.402465192\nclient.shutdown\n&lt;bound method Client.shutdown of &lt;Client: 'tcp://127.0.0.1:50573' processes=5 threads=10, memory=32.00 GiB&gt;&gt;\n\n\nKey points\n\nThe similarity-by-design of the Dask API with pandas makes the transition easy compared to alternatives - although not all functions are replicated.\nScaling up to distributed systems, or down to simply running on your laptop, makes code easily transferable between different resources.\nDask enables parallelism without low level alterations in code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel Python",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn intermediate python skills applied to scientific computing and data science."
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Parallel Python",
    "section": "Trainers",
    "text": "Trainers\n\nKristian Maras (Kris) (MSc Mathematics / Ba Commerce)\nThomas Mauch (Tom) (PhD in astronomy)\nNathaniel (Nate) Butterworth (PhD Computational Geophysics)"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Parallel Python",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nIntroductory Python experience recommended."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Parallel Python",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here."
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Parallel Python",
    "section": "General session timings",
    "text": "General session timings\n\nA. Intoduction and Revise Python Data Manipulation and Pandas Data Structure\nB. Why Polars is a better option for dataframes\nC. Why Dask provides an ecosystem of tools that can run on clusters of machines."
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Parallel Python",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nFor local installation:\ngit clone https://github.com/Sydney-Informatics-Hub/ParallelPython.git\ncd ParallelPython\nconda env create -f environment.yml\nconda activate parallel\nGoogle Colab:\nAlternatively, you can use Google co-lab, which requires you to sign into your google account. Go to Google Colab, and click “new notebook”. Colab is very similar to jupyter notebook except the compute is run on google cloud infrastructure.\nMost packages are by defualt installed. If a package is needed you can run the pip install with the “!” prefix. ie. ! pip install ucimlrepo. This access the underlying terminal."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Go to Google Colab\nSign in to Google Colab if you are not already. Launch “File &gt; New Notebook in Drive”.\nDone.\nIf you would prefer to have a local version of the Python environment we are using, see below for setup instructions.\n\n\nAt home setup\nTo complete the exercises presented in the workshop, you may create a Python environment with the following packages:\ngit clone https://github.com/Sydney-Informatics-Hub/ParallelPython.git\n\ncd ParallelPython\n\nconda env create -f environment.yml\n\nconda activate parallel\nAt the time of this workshop, the major package versions were:\ndask=2022.10.0\njupyterlab=3.4.8\nmatplotlib-base=3.5.3\nnumpy=1.23.4\npandas=1.5.1\npython=3.9.13\nscikit-learn=1.1.2\nscipy=1.9.3\nseaborn=0.12.1\npolars=1.3.0\nOther combinations may also work."
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html",
    "href": "notebooks/01d-dask_delayed.html",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "",
    "text": "Can Dask be used for embarassingly parallel problems?\nHow do you apply it to real functions?\nIn this example we will explore the Schrodinger equation, and how we can use dask for an embarassingly parallel problem.\nSee here for similar problems: https://github.com/natsunoyuki/Computational_Physics_in_Python\nDefine a “computationally intensive” function. Here we are solving for the eigenvalues of \\({\\displaystyle i\\hbar {\\frac {d}{dt}}\\vert Ψ (t)\\rangle ={\\hat {H}}\\vert Ψ (t)\\rangle }\\)\nDefine a function to plot H.\nDefine some potenial energy functions we want to explore.\nLet’s get an idea for how long our schrodinger equation takes to solve.\nEnergy eigenvalues:\n1: 1.00\n2: 3.00\n3: 4.99\n4: 6.99\n5: 8.98\nLet’s profile this function. Is there any way we can speed it up? Or apply some of the techniques we have learned? We can use the iPython/Jupyter magic command %%prun which uses cProfile.\nTLDR: maybe not! Not all code can be “dasked” or parallelised easily.\nOkay. There may not be anything we can improve of greatly. The slowest part is a highly optimised scipy subroutine that is calling fortran under-the-hood! So what if we wanted to run this function 2 times, 3 times, a million times? Perhaps trying different configuration parameters, or specifically here, different potential energy functions."
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html#dask-delayed",
    "href": "notebooks/01d-dask_delayed.html#dask-delayed",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "Dask Delayed",
    "text": "Dask Delayed\nNow let’s try and solve the three variations in parallel. This is an embarassingly parallel problem, as each operation is completely seperate from the other.\nimport dask\n%%time\nlazy_H = []\nfor f in [Vfun1,Vfun2,Vfun3]:\n    H_temp = dask.delayed(schrodinger1D)(f)\n    lazy_H.append(H_temp)\nlazy_H\n%%time \nHH = dask.compute(*lazy_H)\nDone! That is it. You can now run the schrodinger1D as many times as you like in parallel and dask will take of distributing out the work to as many cpus as it can gets its threads on!\n\nChallenge 1\nCan you modify some of the parameters in the schrodinger1D function and see how the timing changes?\n\n\nSolution\n\nTry changing the xmin, xmax, and Nx parameter. These adjust the resolution of the model. You can quickly see how you may want to parallelise this code as each numerical solution can take a long time at high-resolutions.\nxmin = -100\nxmax = 100\nNx = 500\nThen re-run with\n%%time\nH = schrodinger1D(Vfun1)\n\n\n\nExercise 1 Multiple inputs\nCan you re-write the the schrodinger1D function to accept “params” as an argument, then run multiple parameter configurations with a single Potential Energy function?\n\n\nStep 1\n\nModify the schrodinger1D function to accept an additional argument, and pass that argument to the Vfun call.\n#Need to change line 1\ndef schrodinger1D(Vfun, params): \n    ...\n    # And change line 29\n    V = Vfun(x, params = params)\n\n\n\nStep 2\n\nChoose the Vfun you want to explore, and make a list of parameters we want to sweep. I will be looking at Vfun3. A way to make a set of params is to use the product function from the itertools package.\nimport itertools\nparam_config = [[-1,0,1],[-1,0,1],[-1,0,1]]\nparams=list(map(list, itertools.product(*param_config)))\nprint(params)\n[-1, -1, -1]\n[-1, -1, 0]\n[-1, -1, 1]\n[-1, 0, -1]\n[-1, 0, 0]\n[-1, 0, 1]\n[-1, 1, -1]\n[-1, 1, 0]\n[-1, 1, 1]\n[0, -1, -1]\n[0, -1, 0]\n[0, -1, 1]\n[0, 0, -1]\n[0, 0, 0]\n[0, 0, 1]\n[0, 1, -1]\n[0, 1, 0]\n[0, 1, 1]\n[1, -1, -1]\n[1, -1, 0]\n[1, -1, 1]\n[1, 0, -1]\n[1, 0, 0]\n[1, 0, 1]\n[1, 1, -1]\n[1, 1, 0]\n[1, 1, 1]\n\n\n\nStep 3\n\nRe-write the dask delayed function to include your new paramaters.\n%%time\nlazy_H = []\nfor param in params:\n    print(params)\n    H_temp = dask.delayed(schrodinger1D)(Vfun3, param)\n    lazy_H.append(H_temp)\n    \nlazy_H.compute()\n    \n\n\n\nExercise 2 Multiprocessing vs Dask\nHow do you implement this same functionality in native Python Multiprocessing?\n\n\nSolution\n\nThe answer looks something like this:\nwith Pool(processes=ncpus) as pool: \n    y=pool.imap(schrodinger1D, [Vfun1,Vfun2,Vfun3])\n    pool.close()\n    pool.join()\n    outputs = [result for result in y]\nSee the complete solution and description here: schrodinger1D.py\n\n\n\nKey points\n\nDask can be used for embarassingly parallel problems.\nFinding where to make your code faster and understanding what kind of code/data you can determine which approaches you use."
  },
  {
    "objectID": "notebooks/01a-pandas_fundamentals.html",
    "href": "notebooks/01a-pandas_fundamentals.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Recap why pandas has been the go to package for most of python’s history.\nExplore pitfalls of pandas and python in general.\nWhat does parallel mean?"
  },
  {
    "objectID": "notebooks/01a-pandas_fundamentals.html#some-terminology-processes-threads-and-shared-memory",
    "href": "notebooks/01a-pandas_fundamentals.html#some-terminology-processes-threads-and-shared-memory",
    "title": "Python fundamentals",
    "section": "Some terminology: Processes, threads and shared memory",
    "text": "Some terminology: Processes, threads and shared memory\n\nA process is a collection of resources including program files and memory that operates as an independent entity. Since each process has a seperate memory space, it can operate independently from other processes. It cannot easily access shared data in other processes.\nA thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. Threads are considered lightweight because they use far less resources than processes. Threads also share the same memory space so are not independent.\n\n\n\n\n\n\n\n\n\nThe designers of the Python language made the choice that only one thread in a process can run actual Python code by using the so-called global interpreter lock (GIL).\nExternal libraries (NumPy, SciPy, Pandas, etc), written in C or other languages, can release the lock and run multi-threaded. Code writen in native Python has the GIL limitation.\nThe multiprocessing library can be used to release the GIL on native Python code. However, modern packages such as Dask and Polars use it under the hood without specifying pools of resources to be used.\n\nPandas Dataframes - Recap The Staple of Python data manipulation\nPandas has been the standard in Python for many years\n\nHas an extensive set of Features.\nRepresent commond datastructures and operatings.\nEfficiency (when using the optimised c code under the hood).\nWide integration with other open source libraries.\n\nLets recap the Pandas syntax along with an example of a common workflow, focusing on pandas dataframes.\n# Import libraries and datasets\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\n\n#load the data\ndf = sns.load_dataset('diamonds')\ndf.to_csv('diamonds.csv')\ndf.head() #inspect DataFrame\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nDataFrame Functions\nFunctions are available that are attached to the DataFrame class\nCommon methods are:\n\nfilter: Subset the dataframe columns or rows according to the specified index labels.\nassign: assign / mutate new columns in dataframe\nquery: query the columns of a DataFrame with a boolean expression\ngroupby : group dataframe into sections, usually followed by aggregations\napply : Apply a function along an axis of the DataFrame\n\n# A whole bunch of stuff - recap on pandas\n\n# Inspect dataframe attributes -------------------------------- \n\ndf.index # name of index, in this case a range\ndf.columns # variables carat to z\ndf.values # values as a numpy.ndarray\ndf.dtypes # data types of variables\ndf.shape # rows to column structure\ndf.ndim # number of dimensions\n\n\n# filter or create columns ------------------------------------------------\ndf.filter(['cut']) # returns pandas.DataFrame\ndf['cut'] # as opposed to this which returns pandas.Series or df.cut\ndf.filter([\"carat\",\"cut\"]) # filter more than one variables \ndf.filter(regex=  \"^c\") # with regular expressions\ndf.filter([0,2,5], axis=0).head() # filter rows keeping in mind the index\n\ndf.assign(size = 1) #fills same value\ndf = df.assign(size = np.sqrt(df.x ** 2 + df.y ** 2 + df.z ** 2)) #element wise vector addition\n\n\n# subset observations with query ---------------------------------------------------------------- \n# The quotes in query need to be single-outside, double-inside \ndf.query('color == \"E\"') # filter observations by criteria\ndf.query('cut == \"Ideal\" or cut == \"Premium\"') # filter observations with logical expression\ndf.query('cut == \"Ideal\" | cut == \"Premium\"')  # same thing\ndf.query(\"cut.str.match('^G')\") # query doesn't have regex parameter but can be incorporated via str attribute\ndf.query(\"clarity.str.contains('^\\w+1')\")\ndf.query('price &gt; 500') # querying numeric\n\n\n# chaining manipulations\ndf.query('price &lt; 500').head() \n\n(df\n .filter(['carat', 'color'])\n .query('color == \"E\"')\n .head(3))\n\n# Each column in the pd.Dataframe is pd.Series with methods available\ndf.cut    # referenced column on its own\ndf.cut.value_counts()   \ndf.cut.unique()\ndf.carat.mean()\n\n# groupby: splits DataFrame into multiple compartments and returns a group-by object\n# which aggregations can be applied on each group\ndf.groupby('cut').price.agg('std')\ndf.groupby('cut').mean(numeric_only=True)\n\n# apply functions : apply a function to a DataFrame over columns (axis = 1) or rows (axis = 0) \ndf.assign(norm = df.filter(['x','y','z']).apply(np.linalg.norm,axis = 1)) # element / rowwise norm equivalent\n\n# map : very similar to apply but acts on pd.Series rather than pd.DataFrame\ndf.price.map(lambda r : r + 1) #returns a pd.Series\n\n# applymap: Apply a function to a DataDrame element-wise\ndf.filter(['x','y','z']).applymap(lambda x : x **2) \n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_1074/1161367035.py:50: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby('cut').price.agg('std')\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_1074/1161367035.py:51: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby('cut').mean(numeric_only=True)\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_1074/1161367035.py:60: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df.filter(['x','y','z']).applymap(lambda x : x **2)\n\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n0\n15.6025\n15.8404\n5.9049\n\n\n1\n15.1321\n14.7456\n5.3361\n\n\n2\n16.4025\n16.5649\n5.3361\n\n\n3\n17.6400\n17.8929\n6.9169\n\n\n4\n18.8356\n18.9225\n7.5625\n\n\n...\n...\n...\n...\n\n\n53935\n33.0625\n33.1776\n12.2500\n\n\n53936\n32.3761\n33.0625\n13.0321\n\n\n53937\n32.0356\n32.2624\n12.6736\n\n\n53938\n37.8225\n37.4544\n13.9876\n\n\n53939\n33.9889\n34.4569\n13.2496\n\n\n\n\n53940 rows × 3 columns\n\nAlot more functions available in Pandas API. Arguably too many to remember the syntax in an intuitive way.\n\n\nTypical workflow - Reading, Manipulating and Plotting\nThe below follows a generic workflow of reading and manipulation data for insight and visualisation. Note in each assignment calcuations are triggered. As data becomes larger this slows down computation (assuming the data fits in memory).\n\n#reading data\ndf = pd.read_csv('diamonds.csv') \n\n#subseting data\ndf_subset = (df \n             .filter(['cut','carat','clarity','x','y','z','price'])\n             .query('carat &lt; 2.5'))\n\n#manipulating data for insight\npremium_by_clarity = (df.assign(demeaned = df.price - df.groupby('cut').price.transform('mean'))\n  .groupby('clarity').price.mean()\n  .sort_values())\n\npremium_by_clarity\nclarity\nVVS1    2523.114637\nIF      2864.839106\nVVS2    3283.737071\nVS1     3839.455391\nI1      3924.168691\nVS2     3924.989395\nSI1     3996.001148\nSI2     5063.028606\nName: price, dtype: float64\n#visualization\ndf_longer = (df_subset \n    .melt(id_vars=['cut','price','clarity','carat'], \n    value_vars = ['x','y','z'],\n    value_name = \"dim\"\n     )\n)\nsns.relplot(x=\"dim\", y=\"carat\", hue=\"cut\", size=\"price\",\n            sizes=(10, 200), alpha=.5, palette=\"muted\",\n            height=6, data=df_longer.query('dim &lt; 12'));\n\n\n\nChallenge\nMake a plot of the carat vs price, group the colors by the cut and the symbol size by the color of the diamond. Limit the dataset to just show the “I1” clarity.\n\n\nSolution\n\nThis can be done in a few ways, but Seaborn interfaces with pandas-like dataframes seamlessly to make these simple data-manging tasks easy.\nsns.relplot(x=\"carat\", y=\"price\", hue=\"cut\",size='color',\n        sizes=(10, 200), alpha=.5, palette=\"muted\",\n        height=6, data=data.query('clarity == \"I1\"'))\n\n\nSome common issues with this typical workflow are:\n\nData may not fit easily into one dataframe\nEach data manipulation (and reassignment to a variable) triggers computation.\nPandas syntax sometimes difficult to understand and holds too many options to remember.\nPython GIL prevent threads from running at the same time (for now)\n\n\nAside - Generators may be helpful for functions you define yourself\nA Generator in Python is a function that returns an iterator (rather than an actual value/ set of values) using the Yield keyword. Think of it as defining the process rather than processing.\nA few useful links with ideas on how to do this:\n\nhttps://caam37830.github.io/book/index.html\nhttps://python-course.eu/\nhttps://realpython.com/fibonacci-sequence-python/\n\ndef FibonacciGenerator(n):\n    \"\"\" \n    note: n is limit of fibonacci value rather than count\n    \"\"\"\n    a = 0\n    b = 1\n    while a &lt; n:\n        yield a\n        a, b = b, a + b\n        \n\ndef is_even(sequence):\n    \"\"\" reduces a sequence to even numbers\n    \"\"\"\n    for n in sequence:\n        if n % 2 == 0:\n            yield n\n# Can consume generators by converting to list\nlist(is_even([1,2,3,4]))\n[2, 4]\nlist(FibonacciGenerator(500))\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n# Build up sequences of manipulations in memory with generators \n# and selectively trigger consumption with \"list\" for efficiency. \n# Conceptually this is using lazy functions which we will talk more about.\nlist(is_even(FibonacciGenerator(500)))\n[0, 2, 8, 34, 144]\n\n\nKey points\n\nStay within the pandas, numpy ecosystem as much as possible (its c code under the hood)\nConsider building a series of generators\nThe GIL prevents python from utilising multple cores effectively on your machine."
  }
]