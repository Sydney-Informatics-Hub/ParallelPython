[
  {
    "objectID": "notebooks/01a-pandas_fundamentals.html",
    "href": "notebooks/01a-pandas_fundamentals.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Objectives\n\nRecap why pandas has been the go to package for most of python’s history.\nExplore pitfalls of pandas and python in general.\nWhat does parallel mean?\n\n\n\nWhat is Parallel - Why is python slow (sometimes)\nA process is a collection of resources including program files and memory, that operates as an independent entity. Since each process has a seperate memory space, it can operate independently from other processes. It cannot easily access shared data in other processes.\nA thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. Threads are considered lightweight because they use far less resources than processes. Threads also share the same memory space so are not independent.\nBack to python, the multiprocessing library was designed to break down the Global Interpreter Lock (GIL) that limits one thread to control the Python interpreter.\nIn Python, the things that are occurring simultaneously are called by different names (thread, task, process). While they all fall under the definition of concurrency (multiple things happening anaologous to different trains of thought), only multiprocessing actually runs these trains of thought at literally the same time. We will only cover multiprocessing here which assists in CPU bound operations - but keep in mind other methods exist (threading), whose implementation tends to be more low level.\n\n\nPandas Dataframes - Recap The Staple of Python data manipulation\n# Import libraries and datasets\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\nimport dask\n\nts_data = dask.datasets.timeseries()\ndf = sns.load_dataset('diamonds')\ndf.head() #inspect DataFrame\n/Users/kris/miniconda3/envs/parallel/lib/python3.9/site-packages/dask_expr/_collection.py:5968: UserWarning: dask_expr does not support the DataFrameIOFunction protocol for column projection. To enable column projection, please ensure that the signature of `func` includes a `columns=` keyword argument instead.\n  warnings.warn(\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nDataFrame Functions\nFunctions are available that are attached to the DataFrame class\nCommon methods are:\n\nfilter: Subset the dataframe rows or columns according to the specified index labels.\nassign: assign / mutate new columns in dataframe\nquery: query the columns of a DataFrame with a boolean expression\nsort_values : arrange rows of DataFrame\napply : Apply a function along an axis of the DataFrame\n\n# A whole bunch of stuff - recap on pandas\n\n# DataFrame attributes can be accessed\n\ndf.index # name of index, in this case a range\ndf.columns # variables carat to z\ndf.values # values as a numpy.ndarray\ndf.dtypes # data types of variables\ndf.shape # rows to column structure\ndf.ndim # number of dimensions\n\n# functions are attached to pd.Series can be engaged\ndf.cut    # referenced column on its own\ndf.cut.value_counts()   \ndf.cut.unique()\ndf.carat.mean()\n\n# filter variables\ndf.filter(['cut']) # returns pandas.DataFrame\ndf['cut'] # as opposed to this which returns pandas.Series or df.cut\ndf.filter([\"carat\",\"cut\"]) # filter more than one variables \ndf.filter(regex=  \"^c\") # with regex - a whole other topic...\n\n# query observations\n# The quotes in query need to be single-outside, double-inside \ndf.query('color == \"E\"') # filter observations by criteria\ndf.query('cut == \"Ideal\" or cut == \"Premium\"') # filter observations with logical expression\ndf.query('cut == \"Ideal\" | cut == \"Premium\"')  # same thing\ndf.query(\"cut.str.match('^G')\") # query doesn't have regex parameter but can be incorporated via str attribute\ndf.query(\"clarity.str.contains('^\\w+1')\")\ndf.query('price &gt; 500') # querying numeric\n\n# other ways to filter variables or observations by string exist\nsubset = [col for col in df.columns.str.contains('c')] # list comprehension returning list of booleans\ndf.filter(df.columns[subset]) # which are tweaked to filter command\ndf[df.cut.str.startswith('Good')] # subsetting observations \n\n# most DataFrame functions return a DataFrame so one can combine different DataFrame operations\ndf.query('price &lt; 500').head() \n\n# chaining manipulations into larger readable structure\n(df\n .filter(['carat', 'color'])\n .query('color == \"E\"')\n .head(3))\n\n# or using functions applied after a chain\n(df\n    .query('price &lt; 4000')\n    .price.std())\n\n# While we've only so far looked at functions attached to pd.Dataframe,\n# one can use external functions provided what it expects is catered for.\n\nnp.linalg.norm(df.filter(['x','y','z']).values) # norm expects an array\n\n\n# arrange data by values\ndf.sort_values(by = ['carat','price'],ascending = False)\n\n# groupby: splits DataFrame into multiple compartments and returns a group-by object\n# which aggregations can be applied on each group\ndf.groupby('cut').price.agg('std')\ndf.groupby('cut').mean(numeric_only=True)\n\n# Using assign to create / mutate a variable\n\ndf.assign(size = 1) #fills same value\ndf = df.assign(size = np.sqrt(df.x ** 2 + df.y ** 2 + df.z ** 2)) #element wise vector addition\n\n# apply : apply a function to a DataFrame over columns (axis = 1) or rows (axis = 0) \ndf.assign(norm = df.filter(['x','y','z']).apply(np.linalg.norm,axis = 1)) # element / rowwise norm equivalent\n\ndf.assign(demeaned = lambda df : df.price - df.price.mean()) \n\n#if aggregation is based on grouping\ndf_cut = df.groupby('cut')\ndf.assign(demeaned = df.price - df_cut.price.transform('mean')) #transform\n\n# map : Map values of Series according to an input mapping or function.\n# very similar to apply but acts on pd.Series rather than pd.DataFrame\n\ndf.price.map(lambda r : r + 1) #returns a pd.Series\n\n# applymap: Apply a function to a DataDrame element-wise\ndf.filter(['x','y','z']).applymap(lambda x : x **2) \n\n# Reshaping data with melt\n# Melt converts data to long format. \n# pivot is the column equivalent to expand data wider\n''' some melt arguments are :\nid_vars ; Column(s) to use as identifier variables\nvalue_vars ; Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nvar_name ; Name to use for the ‘variable’ column\nvalue_name ; Name to use for the ‘value’ column\n'''\n\ndf_longer = (df.filter(['cut','carat','clarity','x','y','z','price'])\n    .melt(id_vars=['cut','price','clarity','carat'], \n    value_vars = ['x','y','z'],\n    value_name = \"dim\"\n     )\n)\n\n# Longer format usually good for plotting.\n\nsns.relplot(x=\"dim\", y=\"carat\", hue=\"cut\", size=\"price\",\n            sizes=(10, 200), alpha=.5, palette=\"muted\",\n            height=6, data=df_longer.query('dim &lt; 12'));\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_3755/2303346034.py:63: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby('cut').price.agg('std')\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_3755/2303346034.py:64: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df.groupby('cut').mean(numeric_only=True)\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_3755/2303346034.py:77: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  df_cut = df.groupby('cut')\n/var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/ipykernel_3755/2303346034.py:86: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df.filter(['x','y','z']).applymap(lambda x : x **2)\n\n\n\nChallenge\nMake a plot of the carat vs price, group the colors by the cut and the symbol size by the color of the diamond. Limit the dataset to just show the “I1” clarity.\n\n\nSolution\n\nThis can be done in a few ways, but Seaborn interfaces with pandas-like dataframes seamlessly to make these simple data-manging tasks easy.\nsns.relplot(x=\"carat\", y=\"price\", hue=\"cut\",size='color',\n        sizes=(10, 200), alpha=.5, palette=\"muted\",\n        height=6, data=data.query('clarity == \"I1\"'))\n\n\n\nTechniques for writing efficient Python\n\nStay within pandas (structures and functions) rather than function written in pure Python\nConsider generators\n\nA few useful links with ideas on how to do this:\n\nhttps://caam37830.github.io/book/index.html\nhttps://python-course.eu/\nhttps://realpython.com/fibonacci-sequence-python/\n\ndef FibonacciGenerator(n):\n    \"\"\" \n    note: n is limit of fibonacci value rather than count\n    \"\"\"\n    a = 0\n    b = 1\n    while a &lt; n:\n        yield a\n        a, b = b, a + b\n        \n\ndef is_even(sequence):\n    \"\"\" reduces a sequence to even numbers\n    \"\"\"\n    for n in sequence:\n        if n % 2 == 0:\n            yield n\n# Can consume generators by converting to list\nlist(is_even([1,2,3,4]))\n[2, 4]\nlist(FibonacciGenerator(500))\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n# Build up sequences of manipulations in memory with generators \n# and selectively trigger consumption with \"list\" for efficiency. \n# Conceptually this is using lazy functions which we will talk more about.\nlist(is_even(FibonacciGenerator(500)))\n[0, 2, 8, 34, 144]\n\n\nKey points\n\nStay within the pandas, numpy ecosystem as much as possible (its c code under the hood)\nConsider building a series of generators\nThe GIL prevents python from utilising multple cores effectively on your machine."
  },
  {
    "objectID": "notebooks/polars-example.html",
    "href": "notebooks/polars-example.html",
    "title": "Polars - Example",
    "section": "",
    "text": "TO DO - real world data??\n\n# Load the diamonds dataset from seaborn\ndiamonds = sns.load_dataset('diamonds')\n\n# Convert the pandas DataFrame to a polars DataFrame\ndiamonds_df = pl.from_pandas(diamonds)\n\nprint(diamonds_df)\nshape: (53_940, 10)\n┌───────┬───────────┬───────┬─────────┬───┬───────┬──────┬──────┬──────┐\n│ carat ┆ cut ┆ color ┆ clarity ┆ … ┆ price ┆ x ┆ y ┆ z │\n│ — ┆ — ┆ — ┆ — ┆ ┆ — ┆ — ┆ — ┆ — │\n│ f64 ┆ cat ┆ cat ┆ cat ┆ ┆ i64 ┆ f64 ┆ f64 ┆ f64 │\n╞═══════╪═══════════╪═══════╪═════════╪═══╪═══════╪══════╪══════╪══════╡\n│ 0.23 ┆ Ideal ┆ E ┆ SI2 ┆ … ┆ 326 ┆ 3.95 ┆ 3.98 ┆ 2.43 │\n│ 0.21 ┆ Premium ┆ E ┆ SI1 ┆ … ┆ 326 ┆ 3.89 ┆ 3.84 ┆ 2.31 │\n│ 0.23 ┆ Good ┆ E ┆ VS1 ┆ … ┆ 327 ┆ 4.05 ┆ 4.07 ┆ 2.31 │\n│ 0.29 ┆ Premium ┆ I ┆ VS2 ┆ … ┆ 334 ┆ 4.2 ┆ 4.23 ┆ 2.63 │\n│ 0.31 ┆ Good ┆ J ┆ SI2 ┆ … ┆ 335 ┆ 4.34 ┆ 4.35 ┆ 2.75 │\n│ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … │\n│ 0.72 ┆ Ideal ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.75 ┆ 5.76 ┆ 3.5 │\n│ 0.72 ┆ Good ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.69 ┆ 5.75 ┆ 3.61 │\n│ 0.7 ┆ Very Good ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.66 ┆ 5.68 ┆ 3.56 │\n│ 0.86 ┆ Premium ┆ H ┆ SI2 ┆ … ┆ 2757 ┆ 6.15 ┆ 6.12 ┆ 3.74 │\n│ 0.75 ┆ Ideal ┆ D ┆ SI2 ┆ … ┆ 2757 ┆ 5.83 ┆ 5.87 ┆ 3.64 │\n└───────┴───────────┴───────┴─────────┴───┴───────┴──────┴──────┴──────┘\n# random\nimport polars as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the diamonds dataset from seaborn\ndiamonds = sns.load_dataset('diamonds')\n\n# Convert the pandas DataFrame to a polars DataFrame\ndiamonds_df = pl.from_pandas(diamonds)\n\n# Encode categorical variables\ndiamonds_df = diamonds_df.with_column(pl.col(\"cut\").cast(pl.Categorical))\n\n# Split the data into features and target\nX = diamonds_df.select([\"carat\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"]).to_pandas()\ny = diamonds_df.select([\"cut\"]).to_pandas()\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n   carat        cut color clarity  depth  table  price     x     y     z\n0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43\n1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31\n2 0.23 Good E VS1 56.9 65.0 327 4.05 4.07 2.31\n3 0.29 Premium I VS2 62.4 58.0 334 4.20 4.23 2.63\n4 0.31 Good J SI2 63.3 58.0 335 4.34 4.35 2.75\n… … … … … … … … … … …\n53935 0.72 Ideal D SI1 60.8 57.0 2757 5.75 5.76 3.50\n53936 0.72 Good D SI1 63.1 55.0 2757 5.69 5.75 3.61\n53937 0.70 Very Good D SI1 62.8 60.0 2757 5.66 5.68 3.56\n53938 0.86 Premium H SI2 61.0 58.0 2757 6.15 6.12 3.74\n53939 0.75 Ideal D SI2 62.2 55.0 2757 5.83 5.87 3.64\n[53940 rows x 10 columns]"
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html",
    "href": "notebooks/01d-dask_delayed.html",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "",
    "text": "Can Dask be used for embarassingly parallel problems?\nHow do you apply it to real functions?\nIn this example we will explore the Schrodinger equation, and how we can use dask for an embarassingly parallel problem.\nSee here for similar problems: https://github.com/natsunoyuki/Computational_Physics_in_Python\nDefine a “computationally intensive” function. Here we are solving for the eigenvalues of \\({\\displaystyle i\\hbar {\\frac {d}{dt}}\\vert Ψ (t)\\rangle ={\\hat {H}}\\vert Ψ (t)\\rangle }\\)\nDefine a function to plot H.\nDefine some potenial energy functions we want to explore.\nLet’s get an idea for how long our schrodinger equation takes to solve.\nEnergy eigenvalues:\n1: 1.00\n2: 3.00\n3: 4.99\n4: 6.99\n5: 8.98\nLet’s profile this function. Is there any way we can speed it up? Or apply some of the techniques we have learned? We can use the iPython/Jupyter magic command %%prun which uses cProfile.\nTLDR: maybe not! Not all code can be “dasked” or parallelised easily.\nOkay. There may not be anything we can improve of greatly. The slowest part is a highly optimised scipy subroutine that is calling fortran under-the-hood! So what if we wanted to run this function 2 times, 3 times, a million times? Perhaps trying different configuration parameters, or specifically here, different potential energy functions."
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html#dask-delayed",
    "href": "notebooks/01d-dask_delayed.html#dask-delayed",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "Dask Delayed",
    "text": "Dask Delayed\nNow let’s try and solve the three variations in parallel. This is an embarassingly parallel problem, as each operation is completely seperate from the other.\nimport dask\n%%time\nlazy_H = []\nfor f in [Vfun1,Vfun2,Vfun3]:\n    H_temp = dask.delayed(schrodinger1D)(f)\n    lazy_H.append(H_temp)\nlazy_H\n%%time \nHH = dask.compute(*lazy_H)\nDone! That is it. You can now run the schrodinger1D as many times as you like in parallel and dask will take of distributing out the work to as many cpus as it can gets its threads on!\n\nChallenge 1\nCan you modify some of the parameters in the schrodinger1D function and see how the timing changes?\n\n\nSolution\n\nTry changing the xmin, xmax, and Nx parameter. These adjust the resolution of the model. You can quickly see how you may want to parallelise this code as each numerical solution can take a long time at high-resolutions.\nxmin = -100\nxmax = 100\nNx = 500\nThen re-run with\n%%time\nH = schrodinger1D(Vfun1)\n\n\n\nExercise 1 Multiple inputs\nCan you re-write the the schrodinger1D function to accept “params” as an argument, then run multiple parameter configurations with a single Potential Energy function?\n\n\nStep 1\n\nModify the schrodinger1D function to accept an additional argument, and pass that argument to the Vfun call.\n#Need to change line 1\ndef schrodinger1D(Vfun, params): \n    ...\n    # And change line 29\n    V = Vfun(x, params = params)\n\n\n\nStep 2\n\nChoose the Vfun you want to explore, and make a list of parameters we want to sweep. I will be looking at Vfun3. A way to make a set of params is to use the product function from the itertools package.\nimport itertools\nparam_config = [[-1,0,1],[-1,0,1],[-1,0,1]]\nparams=list(map(list, itertools.product(*param_config)))\nprint(params)\n[-1, -1, -1]\n[-1, -1, 0]\n[-1, -1, 1]\n[-1, 0, -1]\n[-1, 0, 0]\n[-1, 0, 1]\n[-1, 1, -1]\n[-1, 1, 0]\n[-1, 1, 1]\n[0, -1, -1]\n[0, -1, 0]\n[0, -1, 1]\n[0, 0, -1]\n[0, 0, 0]\n[0, 0, 1]\n[0, 1, -1]\n[0, 1, 0]\n[0, 1, 1]\n[1, -1, -1]\n[1, -1, 0]\n[1, -1, 1]\n[1, 0, -1]\n[1, 0, 0]\n[1, 0, 1]\n[1, 1, -1]\n[1, 1, 0]\n[1, 1, 1]\n\n\n\nStep 3\n\nRe-write the dask delayed function to include your new paramaters.\n%%time\nlazy_H = []\nfor param in params:\n    print(params)\n    H_temp = dask.delayed(schrodinger1D)(Vfun3, param)\n    lazy_H.append(H_temp)\n    \nlazy_H.compute()\n    \n\n\n\nExercise 2 Multiprocessing vs Dask\nHow do you implement this same functionality in native Python Multiprocessing?\n\n\nSolution\n\nThe answer looks something like this:\nwith Pool(processes=ncpus) as pool: \n    y=pool.imap(schrodinger1D, [Vfun1,Vfun2,Vfun3])\n    pool.close()\n    pool.join()\n    outputs = [result for result in y]\nSee the complete solution and description here: schrodinger1D.py\n\n\n\nKey points\n\nDask can be used for embarassingly parallel problems.\nFinding where to make your code faster and understanding what kind of code/data you can determine which approaches you use."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Go to Google Colab\nSign in to google if you are not already. Launch “File &gt; New Notebook in Drive”.\nDone. If you would prefer to have a local version of the Python environment we are using, see below for setup instructions.\n\n\nAt home setup\nTo complete the exercises presented in the workshop, you may create a Python environment with the following packages:\ngit clone https://github.com/Sydney-Informatics-Hub/ParallelPython.git\n\ncd ParallelPython\n\nconda env create -f environment.yml\n\nconda activate parallel\nAt the time of this workshop, the major package versions were:\ndask=2022.10.0\njupyterlab=3.4.8\nmatplotlib-base=3.5.3\nnumpy=1.23.4\npandas=1.5.1\npython=3.9.13\nscikit-learn=1.1.2\nscipy=1.9.3\nseaborn=0.12.1\nOther combinations may also work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel Python",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn intermediate python skills applied to scientific computing and data science."
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Parallel Python",
    "section": "Trainers",
    "text": "Trainers\n\nKristian Maras (Kris) (MSc Mathematics / Ba Commerce)\nThomas Mauch (Thomas) (PhD in astronomy)\nNathaniel (Nate) Butterworth (PhD Computational Geophysics)"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Parallel Python",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nIntroductory Python experience recommended."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Parallel Python",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here."
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Parallel Python",
    "section": "General session timings",
    "text": "General session timings\n\nA. Intoduction and Revise Python Data Manipulation and Pandas Data Structure\nB. Why Polars is a better option for dataframes\nC. Why Dask provides an ecosystem of tools that can run on clusters of machines."
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Parallel Python",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nFor local installation:\ngit clone https://github.com/Sydney-Informatics-Hub/ParallelPython.git\ncd ParallelPython\nconda env create -f environment.yml\nconda activate parallel\nGoogle Colab:\nAlternatively, you can use Google co-lab, which requires you to sign into your google account. Go to https://colab.research.google.com/, and click “new notebook”. colab is very similar to jupyter notebook except the compute is run on google cloud infrastructure.\nMost packages are by defualt installed. If a package is needed you can run the pip install with the “!” prefix. ie. ! pip install ucimlrepo."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html",
    "href": "notebooks/01b-TransitionToDask.html",
    "title": "What is Dask?",
    "section": "",
    "text": "Dask\n10 minute intro\nAPI Reference\nDask dataframes process data in parallel across multiple cores or machines.\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index.\nDask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\nInternally, a Dask DataFrame is split into many partitions, where each partition is one Pandas DataFrame. When our index is sorted and we know the values of the divisions of our partitions, then we can be clever and efficient with expensive algorithms (e.g. groupby’s, joins, etc…).\nUse Cases:\nDask DataFrame is used in situations where pandas is commonly needed, usually when pandas fails due to data size or speed of computation. Common use cases are:\nDask DataFrame may not be the best choice in the following situations:"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "title": "What is Dask?",
    "section": "Transitioning to Dask DataFrames",
    "text": "Transitioning to Dask DataFrames\nDask mimics most but not all of the well known pandas API syntax.\n# IO ---------------------------------------------------------------- \n  \n# loading into a dask dataframe\nddf = dd.read_csv('diamonds.csv') #reading multiple files with patterns dd.read_csv('d*.csv')\nddf = dd.from_pandas(df,npartitions = 2) \n\n# by default it has lazy execution where computation are triggered by compute() (or head) \nddf.compute() # convert dd to pd.DataFrame\nddf.head(2)\n\n# Inspecting ----------------------------------------------------------------\nddf.npartitions # number of partitions\nddf.divisions # Divisions includes the minimum value of every partition’s index and the maximum value of the last partition’s index\nddf.partitions[1] # access a particular partition\nddf.partitions[1].index # which have similar pd.DataFrame attributes\nddf.describe().compute() # general stats\nddf.dtypes  # access attributes\n\n# Columns \nddf[[\"carat\",\"price\"]] # subset columns\nddf['new_column'] = ddf['carat'] * ddf['depth'] # creating new column\n\n# Filtering rows ----------------------------------------------------------------\n\nddf.query('price &gt; 50') # same as pd.DataFrame\n\nddf.loc[15:20] # subseing rows \n\n# Group by Aggregations ----------------------------------------------------------------\n\n# By default, groupby methods return an object with only 1 partition. \n# This is to optimize performance, and assumes the groupby reduction returns an object that is small enough to fit into memory. \n# If your returned object is larger than this, you can increase the number of output partitions using the split_out argument.\nddf.groupby('cut').price.mean() #npartitions=1\n#ddf.groupby('cut').mean(split_out=2) #npartitions=2\n\n# dask aggregate has more features than pandas agg equivalent, supports reductions on the same group.\nddf_aggs = (ddf.groupby('cut')\n    .aggregate({\"price\":\"mean\",\"carat\":\"sum\"})).compute()\nddf_aggs\n\n\n\n\n\n\n\nprice\ncarat\n\n\ncut\n\n\n\n\n\n\nIdeal\n3457.541970\n15146.84\n\n\nPremium\n4584.257704\n12300.95\n\n\nGood\n3928.864452\n4166.10\n\n\nVery Good\n3981.759891\n9742.70\n\n\nFair\n4358.757764\n1684.28"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#contolling-laziness-and-optimising-with-dask",
    "href": "notebooks/01b-TransitionToDask.html#contolling-laziness-and-optimising-with-dask",
    "title": "What is Dask?",
    "section": "Contolling Laziness and Optimising with Dask",
    "text": "Contolling Laziness and Optimising with Dask\n\n#controlling when execution occurs is key to performance\nlazy_manipulations = (ddf.query('price &gt; 50').\n    groupby('clarity').\n    price.mean())\nlazy_manipulations.compute() # trigger computation to pd.DataFrame\n\n\n# Can persist data into RAM if possible making future operations on it faster\nddf_aggs = ddf_aggs.repartition(npartitions = 1).persist()\n\n\n\n\n\n\n\n\ncarat_original\ncut\ncolor\nclarity\ndepth\ntable\nprice_original\nx\ny\nz\nprice_aggregated\ncarat_aggregated\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n3457.54197\n15146.84\n\n\n11\n0.23\nIdeal\nJ\nVS1\n62.8\n56.0\n340\n3.93\n3.90\n2.46\n3457.54197\n15146.84\n\n\n\n\n\nNote that not all apis from pandas are available in Dask. For example, ddf.filter(['carat','price']) is not available. For more details and a list of available options, see here.\n\nChallenge\n\nWhat is the price per carat over the entire dataset?\nCreate a column called price_to_carat that calculates this for each row\nCreate a column called expensive that flags whether price is greater than price_to_carat\nHow many expensive diamonds are there\n\n\n\nSolution\n\n\nAverage price to carat $4928\n15003 expensive diamonds compared to whole dataset\n\nprice_per_carat = (ddf.price.sum() / ddf.carat.sum()).compute()\n\nddf = ddf.assign(price_to_carat = ddf.price / ddf.carat)\n\ndef greater_than_avg(price):\n    if price &gt; price_per_carat:\n        return True\n    else:\n        return False\n\nddf = ddf.assign(expensive = ddf.price.apply(greater_than_avg))\nddf.sort_values('expensive',ascending= False).compute()\nnumber_expensive = ddf.expensive.sum().compute()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "href": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "title": "What is Dask?",
    "section": "Dask Best Practice Guide",
    "text": "Dask Best Practice Guide\n\nUse set_index() sparingly to speed up data naturally sorted on a single index\n\nUse ddf.set_index('column')\n\nPersist intelligently\n\n\nIf you have the available RAM for your dataset then you can persist data in memory. On distributed systems, it is a way of telling the cluster that it should start executing the computations that you have defined so far, and that it should try to keep those results in memory.\n\ndf = df.persist()\n\n\n\nRepartition to reduce overhead\n\nAs you reduce or increase the size of your pandas DataFrames by filtering or joining, it may be wise to reconsider how many partitions you need. Adjust partitions accordingly using repartition.\ndf = df.repartition(npartitions=df.npartitions // 100)\n\nConsider storing large data in Apache Parquet Format (binary column based format)"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#additional-info-on-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#additional-info-on-dask-dataframes",
    "title": "What is Dask?",
    "section": "Additional Info on Dask DataFrames",
    "text": "Additional Info on Dask DataFrames\nTo learn more on how to use dask dataframes, feel free to go through Reading Messy Data Binder at your own pace.This example demostrates both dask and using delayed functions. Otherwise here is more info including best practices and the API, and other tutorials on dask is also available."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "href": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "title": "What is Dask?",
    "section": "Dask Arrays",
    "text": "Dask Arrays\nimport dask.array as da\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nx\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n762.94 MiB\n7.63 MiB\n\n\nShape\n(10000, 10000)\n(1000, 1000)\n\n\nCount\n1 Graph Layer\n100 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n# numpy syntax as usual\ny = x + x.T\nz = y[::2, 5000:].mean(axis=1) # axis 0 is index, axis 1 is columns\nz\n# Trigger compute and investigate Client\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n39.06 kiB\n3.91 kiB\n\n\nShape\n(5000,)\n(500,)\n\n\nCount\n7 Graph Layers\n10 Chunks\n\n\nType\nfloat64\nnumpy.ndarray"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "href": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "title": "What is Dask?",
    "section": "Diagnostics - Profile resource efficiency in real time",
    "text": "Diagnostics - Profile resource efficiency in real time\nThe Dask Dashboard enables resource monitoring across RAM, CPU, workers, threads and tasks (functions), in real time as your code is running. It opens a user friendly dashboard allowing you to inspect what processes are running and what work (or bottlenecks) exist across your cores.\nSee here for documentation and videos.\n\nA few key definitions:\n\nBytes Stored and Bytes per Worker: Cluster memory and Memory per worker.\nTask Processing/CPU Utilization/Occupancy: Tasks being processed by each worker/ CPU Utilization per worker/ Expected runtime for all tasks currently on a worker.\nProgress: Progress of a set of tasks.\n\nThere are three different colors of workers in a task graph:\n\nBlue: Processing tasks.\nGreen: Saturated: It has enough work to stay busy.\nRed: Idle: Does not have enough work to stay busy.\nTask Stream: Individual task across threads.\n\nWhite colour represents deadtime.\n\n\n# To load diagnostic in web browser on local. Wont work on CoLab.\nfrom dask.distributed import Client\nclient = Client()\nclient #client.shutdown after use\n\n     \n    \n        Client\n        Client-a102ceae-4a3f-11ef-890e-d687d7367289\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\n40a61b40\n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 5\n\n\nTotal threads: 10\nTotal memory: 32.00 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-bd6ab190-cc77-4a45-9877-db3c9fae6eb1\n\n\n\nComm: tcp://127.0.0.1:50944\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 10\n\n\nStarted: Just now\nTotal memory: 32.00 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: tcp://127.0.0.1:50960\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50967/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50947\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-41zn61zx\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: tcp://127.0.0.1:50963\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50968/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50948\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-91692av2\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: tcp://127.0.0.1:50961\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50964/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50949\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-kvhj80x5\n\n\n\n\n\n\n\n\n\n\nWorker: 3\n\n\n\nComm: tcp://127.0.0.1:50959\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50966/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50950\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-k75empbg\n\n\n\n\n\n\n\n\n\n\nWorker: 4\n\n\n\nComm: tcp://127.0.0.1:50962\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:50965/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:50951\n\n\n\nLocal directory: /var/folders/j2/6znnc26n7r9gk1qjgh7wy6bh0000gn/T/dask-scratch-space/worker-q_dwp3u5"
  },
  {
    "objectID": "notebooks/polars-intro.html",
    "href": "notebooks/polars-intro.html",
    "title": "Polars - Getting to know the Syntax",
    "section": "",
    "text": "Polars is a lightning fast DataFrame library. The key features of polars are:\nFast and Accessible: Written from scratch in Rust, designed close to the machine and without external dependencies. It also has python and R bindings!\nI/O: First class support for all common data storage layers: local, cloud storage & databases.\nHandle Datasets larger than RAM\nIntuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.\nOut of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time\nParallel: Utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration.\nThe philosophy of Polars is to provide a dataframe library that utilises available cores, has an intuitive api and is performant - hence adheres to a strict schema (data-types should be known before running the query)."
  },
  {
    "objectID": "notebooks/polars-intro.html#learning-the-syntax",
    "href": "notebooks/polars-intro.html#learning-the-syntax",
    "title": "Polars - Getting to know the Syntax",
    "section": "Learning the Syntax",
    "text": "Learning the Syntax\n# selecting columns ----------------------------------------------------------------\ndf.select(pl.col(\"float\")) # selecting a column\n\ndf.select(pl.col(\"date\",\"string\")) # selecting multiple columns \n\ndf.select(pl.col(\"*\").exclude(\"string\")) # select all columns then exclude\n\ndf.select(pl.col(\"^(da|fl).*$\")) # supports regular expressions\n\nimport polars.selectors as cs   # selectors are helper funtions for selecting columns\n\ndf.select(cs.integer() | cs.contains(\"ate\")) # select all columns that are integers or contains ate\n\n# filtering rows ---------------------------------------------------------------------\ndf.filter(pl.col(\"integer\") &gt;= 2)  #filtering rows\n\ndf.filter((pl.col(\"integer\") &gt;=2) & \n          (pl.col(\"float\") == 5.0)) #filtering rows with multiple conditions (| = or & = and)\n\n#creating / manipulating columns -----------------------------------------------------\ndf.with_columns((pl.col(\"integer\") + 3).alias(\"new_column\")) # creating column and naming it\n\n#group by aggregations ----------------------------------------------------------------\ndf.group_by(\"string\").agg(pl.col(\"integer\").sum().alias(\"sum\"),\n                          pl.col(\"date\").sort().first().alias(\"earliest\"), \n                          pl.col(\"float\") / pl.col(\"integer\")) \n                                \n\nshape: (3, 4)\n\n\n\nstring\nsum\nearliest\nfloat\n\n\nstr\ni64\ndatetime[μs]\nlist[f64]\n\n\n\n\n\"a\"\n1\n2025-01-01 00:00:00\n[4.0]\n\n\n\"b\"\n6\n2025-01-02 00:00:00\n[2.5, 3.0]\n\n\n\"c\"\n3\n2025-01-03 00:00:00\n[2.0]\n\n\n\n\n# Can combine expressions for compactness\n\ndf3 = df.with_columns((pl.col(\"float\") * pl.col(\"integer\"))\n                .alias(\"product\")).select(pl.all().exclude(\"integer\"))\n\nFor more information here are some links to relevant documentation:\nSelecting columns\nAggregation\nData Transformation such as join, Concatenation, pivot and unpivot.\nFunctions for expressions.\nCheck out the API documentation for more information\n\n\nData types and casting\nUnder the hood polars use Arrow data types and memory arrays and offer support for String, Numberic, Nested, Temporay and other types. Most data types are specified by the arrow syntax with the exception of String, Categorical and Object types.\nCategorical data represents string data where the values in the column have a finite set of values (yet for performance implementation different to strings). Polars supports both Enum data type, where categories are known up front, and the more flexible Categorical data type where values are not known beforehand. Conversion between them is trivial. Relying on polars inferring the categories with Categorical types comes at a performance cost (as the encoding is a dictionary like object). See Categorical page for more information.\nCasting (changing the datatypes) is enabled by either specifying the dtype argument or applying the cast() function.\n# Use Enum where categories are known\ncat_types = pl.Enum([\"polar\",\"panda\",\"teddy\"])\nanimals = pl.Series([\"polar\",\"polar\",\"teddy\",\"panda\"],dtype= cat_types)\n# Use Categprical otherwise\nfictional_animals = pl.Series([\"poobear\",\"minimouse\",\"teddy\",\"poobear\"],dtype= pl.Categorical)\n\n# casting columns to other data types with cast\ndf.cast({\"integer\": pl.Float32, \"float\": pl.UInt8})"
  },
  {
    "objectID": "notebooks/polars-intro.html#lazy-eager-api",
    "href": "notebooks/polars-intro.html#lazy-eager-api",
    "title": "Polars - Getting to know the Syntax",
    "section": "Lazy / Eager API",
    "text": "Lazy / Eager API\nPolars supports two modes of operation: lazy and eager. In the eager API the query is executed immediately while in the lazy API the query is only evaluated once it is ‘needed’. Deferring the execution to the last minute can have significant performance advantages.\nBefore we explore the two run this code to fetch the iris dataset\n# fetch iris dataset and save as csv in local path\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\niris = fetch_ucirepo(id=53) \nX = iris.data.features \ny = iris.data.targets \niris_df = pd.concat([X, y], axis=1)\niris_df.rename(columns={'class': 'species'}, inplace=True)\niris_df.to_csv(\"iris_data.csv\", index=False)\nIn this example we use the eager API we\n1. Read the iris dataset.\n2. Filter the dataset based on sepal length\n3. Calculate the mean of the sepal width per species\nEvery step is executed immediately returning the intermediate results. This can be very wasteful as we might do work or load extra data that is not being used.\ndf = pl.read_csv(\"iris_data.csv\")\ndf_small = df.filter(pl.col(\"sepal length\") &gt; 5)\ndf_agg = df_small.group_by(\"species\").agg(pl.col(\"sepal width\").mean())\nIf we instead used the lazy API and waited on execution until all the steps are defined then the query planner could perform various optimizations. In this case:\n\nPredicate pushdown: Apply filters as early as possible while reading the dataset, thus only reading rows with sepal length greater than 5.\nProjection pushdown: Select only the columns that are needed while reading the dataset, thus removing the need to load additional columns (e.g. petal length & petal width)\n\nq = (\n    pl.scan_csv(\"iris_data.csv\") #doesnt read it all before other operation is performed\n        .filter(pl.col(\"sepal length\") &gt; 5)\n        .group_by(\"species\").agg(pl.col(\"sepal width\").mean())\n)\n\nq # a lazyframe\n\ndf_agg = q.collect() # inform polars that you want to execute the query\ndf_agg = q.collect(streaming=True) # with streaming mode\nStreaming\nOne additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once Polars can execute the query in batches allowing you to process datasets that are larger-than-memory.\nTo tell Polars we want to execute a query in streaming mode we pass the streaming=True argument to collect. See here for more info on streaming.\nWhen to use Lazy versus Eager:\nIn general the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don’t know yet what your query is going to look like\n\nTypical pipeline Demo\nGiven your new knowledge of polars, here is an example on how to integrate into the usual pipeline consisting of:\n\nData ingestion and manipulation.\nData visualization\nModel Preperation - Training and Testing\nPrediction\n\n\n# Do you aggregating and manipulation to clean data\n# keep \"as lazy\" as possible\nq = (\n    pl.scan_csv(\"iris_data.csv\") # data manipulation stuff - trivial example\n        .filter(pl.col(\"sepal length\") &gt; 2)\n)\n\niris_df = q.collect() #trigger computation on and store the polars dataframe"
  },
  {
    "objectID": "notebooks/polars-intro.html#ecosystem",
    "href": "notebooks/polars-intro.html#ecosystem",
    "title": "Polars - Getting to know the Syntax",
    "section": "Ecosystem",
    "text": "Ecosystem\nOn this page you can find a non-exhaustive list of libraries and tools that support Polars. As the data ecosystem is evolving fast, more libraries will likely support Polars in the future. One of the main drivers is that Polars makes adheres its memory layout to the Apache Arrow spec.\n# Polars dataframs has kMachine learning \n\n# Example preprocessing with Polars: Adding a new feature\niris_df = df.with_columns(\n    (df['sepal length'] * df['sepal width']).alias('sepal area')\n)\n\n# Separate features and target for scikit-learn\nX = iris_df.select(['sepal length', 'sepal width', 'petal length', 'petal width', 'sepal area'])\ny = iris_df.select('species').to_series()"
  },
  {
    "objectID": "notebooks/01c-scientific_computing.html",
    "href": "notebooks/01c-scientific_computing.html",
    "title": "Background: What is Parallel Python?",
    "section": "",
    "text": "Parallel computing is when many different tasks are carried out simultaneously. Python does this by creating independent processes that ship data, program files and libraries to an isolated ecosystem where computation is performed. There are three main models for parallel computing:"
  },
  {
    "objectID": "notebooks/01c-scientific_computing.html#some-terminology-processes-threads-and-shared-memory",
    "href": "notebooks/01c-scientific_computing.html#some-terminology-processes-threads-and-shared-memory",
    "title": "Background: What is Parallel Python?",
    "section": "Some terminology: Processes, threads and shared memory",
    "text": "Some terminology: Processes, threads and shared memory\n\nA process is a collection of resources including program files and memory that operates as an independent entity. Since each process has a seperate memory space, it can operate independently from other processes. It cannot easily access shared data in other processes.\nA thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. Threads are considered lightweight because they use far less resources than processes. Threads also share the same memory space so are not independent.\n\n\n\n\n\n\n\n\n\nThe designers of the Python language made the choice that only one thread in a process can run actual Python code by using the so-called global interpreter lock (GIL).\nExternal libraries (NumPy, SciPy, Pandas, etc), written in C or other languages, can release the lock and run multi-threaded. Code writen in native Python has the GIL limitation.\nThe multiprocessing library can be used to release the GIL on native Python code.\nimport dask\nimport dask.distributed as dd\nimport dask.array as da\nimport dask.dataframe as dd\nimport pandas as pd\nimport random\nfrom multiprocessing import Pool ### The default pool makes one process per CPU\n\nSpeed up a function that could take a while to run\n\n# reference:\n# https://aaltoscicomp.github.io/python-for-scicomp/parallel/\n\ndef sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n# Using apply from pandas\nps = pd.Series([10**5,20**5])\nps.apply(sample)\n0    3.13884\n1    3.14272\ndtype: float64\n# Create a pool object from with a with statement \nwith Pool() as p:\n    result = p.map(sample,ps)\n    # will engage p.close() automatically\nProcess SpawnPoolWorker-1:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-2:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-10:\nProcess SpawnPoolWorker-8:\nProcess SpawnPoolWorker-11:\nProcess SpawnPoolWorker-9:\nProcess SpawnPoolWorker-3:\nProcess SpawnPoolWorker-7:\nProcess SpawnPoolWorker-4:\nProcess SpawnPoolWorker-12:\nProcess SpawnPoolWorker-6:\nProcess SpawnPoolWorker-5:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt: \nMultiprocessing introduces an initial fixed cost in time (creating Pool objects). Knowing what hardware you are working on is needed to tailor the number of processes created with what is available. There is a risk of creating too many processes which make the initial fixed cost excessively large.\n\n\nWe will come back to the below alternative (Dask) afterwards\n# Create the dask equivalent input\nds = dd.from_pandas(ps,npartitions = 2)\n%%timeit #605 ms ± 10.9 ms per loop\nresult = ds.apply(sample,meta=('x', 'float64')).mean().compute()\n894 ms ± 9.68 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n%%timeit # 1.08 s ± 47.8 ms per loop\np = Pool()\nresult = p.map(sample,ps)\np.close()\nProcess SpawnPoolWorker-13:\nProcess SpawnPoolWorker-14:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-20:\nProcess SpawnPoolWorker-24:\nProcess SpawnPoolWorker-21:\nProcess SpawnPoolWorker-23:\nProcess SpawnPoolWorker-16:\nProcess SpawnPoolWorker-22:\nProcess SpawnPoolWorker-17:\nTraceback (most recent call last):\nProcess SpawnPoolWorker-19:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nProcess SpawnPoolWorker-15:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nProcess SpawnPoolWorker-18:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt: \n\n\nCompare Multiprocessing to Dask\nDask uses multiprocessing by default to overcome the GIL. Hence comparing the run time of the multiprocessing library to Dask with a function-bound problem will yield similar results.\nYet dask offers an ecosystem of resource management (Scheduler, diagnostics, data partitions and Task Graphs) that make it a more attractive way to achieve the same thing in most cases. Resource management is handled automatically by the scheduler.\n# for reference, delaying the same function.\n@dask.delayed\ndef dd_sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n\n\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\nresult\n3.1386374999999997\n%%timeit #595 ms ± 1.54 ms per loop\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\n855 ms ± 7.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nKey points\n\nUsing Multiprocessing (or mpi4py - not covered here) are the traditional ways to make functions run in parallel in Python\nUsing Dask and its ecosystem is the modern approach"
  }
]