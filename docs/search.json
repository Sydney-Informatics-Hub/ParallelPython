[
  {
    "objectID": "notebooks/01a-pandas_fundamentals.html",
    "href": "notebooks/01a-pandas_fundamentals.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Objectives\n\nLearn the basic Pandas data structures and methods\nExplore a typical data manipulation and plotting process\n\n\n\nReferences\nPandas API Reference\n# Import libraries and datasets\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\nimport dask\n\nts_data = dask.datasets.timeseries()\ndf = sns.load_dataset('diamonds')\ndf.head() #inspect DataFrame\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n# DataFrame attributes can be accessed\n\ndf.index # name of index, in this case a range\ndf.columns # variables carat to z\ndf.values # values as a numpy.ndarray\ndf.dtypes # data types of variables\ndf.shape # rows to column structure\ndf.ndim # number of dimensions\n\n# functions are attached to pd.Series can be engaged\ndf.cut    # referenced column on its own\ndf.cut.value_counts()   \ndf.cut.unique()\ndf.carat.mean()\n0.7979397478680014\n\n\nDataFrame Functions\nFunctions are available that are attached to the DataFrame class\nCommon methods are:\n\nfilter: Subset the dataframe rows or columns according to the specified index labels.\nassign: assign / mutate new columns in dataframe\nquery: query the columns of a DataFrame with a boolean expression\nsort_values : arrange rows of DataFrame\napply : Apply a function along an axis of the DataFrame\n\n# More complex subsetting of DataFrame by observations or variables\n\n# filter variables\ndf.filter(['cut']) # returns pandas.DataFrame\ndf['cut'] # as opposed to this which returns pandas.Series or df.cut\ndf.filter([\"carat\",\"cut\"]) # filter more than one variables \ndf.filter(regex=  \"^c\") # with regex - a whole other topic...\n\n# query observations\n# The quotes in query need to be single-outside, double-inside \ndf.query('color == \"E\"') # filter observations by criteria\ndf.query('cut == \"Ideal\" or cut == \"Premium\"') # filter observations with logical expression\ndf.query('cut == \"Ideal\" | cut == \"Premium\"')  # same thing\ndf.query(\"cut.str.match('^G')\") # query doesn't have regex parameter but can be incorporated via str attribute\ndf.query(\"clarity.str.contains('^\\w+1')\")\ndf.query('price &gt; 500') # querying numeric\n\n# other ways to filter variables or observations by string exist\nsubset = [col for col in df.columns.str.contains('c')] # list comprehension returning list of booleans\ndf.filter(df.columns[subset]) # which are tweaked to filter command\ndf[df.cut.str.startswith('Good')] # subsetting observations \n\n# most DataFrame functions return a DataFrame so one can combine different DataFrame operations\ndf.query('price &lt; 500').head() \n\n# chaining manipulations into larger readable structure\n(df\n .filter(['carat', 'color'])\n .query('color == \"E\"')\n .head(3))\n\n# or using functions applied after a chain\n(df\n    .query('price &lt; 4000')\n    .price.std())\n\n# While we've only so far looked at functions attached to pd.Dataframe,\n# one can use external functions provided what it expects is catered for.\n\nnp.linalg.norm(df.filter(['x','y','z']).values) # norm expects an array\n\n2094.3009834069335\n\n# arrange data by values\ndf.sort_values(by = ['carat','price'],ascending = False)\n\n# groupby: splits DataFrame into multiple compartments and returns a group-by object\n# which aggregations can be applied on each group\ndf.groupby('cut').price.agg('std')\ndf.groupby('cut').mean(numeric_only=True)\n\n# Using assign to create / mutate a variable\n\ndf.assign(size = 1) #fills same value\ndf = df.assign(size = np.sqrt(df.x ** 2 + df.y ** 2 + df.z ** 2)) #element wise vector addition\n\n# apply : apply a function to a DataFrame over columns (axis = 1) or rows (axis = 0) \ndf.assign(norm = df.filter(['x','y','z']).apply(np.linalg.norm,axis = 1)) # element / rowwise norm equivalent\n\ndf.assign(demeaned = lambda df : df.price - df.price.mean()) \n\n#if aggregation is based on grouping\ndf_cut = df.groupby('cut')\ndf.assign(demeaned = df.price - df_cut.price.transform('mean')) #transform\n\n# map : Map values of Series according to an input mapping or function.\n# very similar to apply but acts on pd.Series rather than pd.DataFrame\n\ndf.price.map(lambda r : r + 1) #returns a pd.Series\n\n# applymap: Apply a function to a DataDrame element-wise\ndf.filter(['x','y','z']).applymap(lambda x : x **2) \n\n# Reshaping data with melt\n# Melt converts data to long format. \n# pivot is the column equivalent to expand data wider\n''' some melt arguments are :\nid_vars ; Column(s) to use as identifier variables\nvalue_vars ; Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.\nvar_name ; Name to use for the ‘variable’ column\nvalue_name ; Name to use for the ‘value’ column\n'''\n\ndf_longer = (df.filter(['cut','carat','clarity','x','y','z','price'])\n    .melt(id_vars=['cut','price','clarity','carat'], \n    value_vars = ['x','y','z'],\n    value_name = \"dim\"\n     )\n)\n\n# Longer format usually good for plotting.\n\nsns.relplot(x=\"dim\", y=\"carat\", hue=\"cut\", size=\"price\",\n            sizes=(10, 200), alpha=.5, palette=\"muted\",\n            height=6, data=df_longer.query('dim &lt; 12'));\n\n\n\nChallenge\nMake a plot of the carat vs price, group the colors by the cut and the symbol size by the color of the diamond. Limit the dataset to just show the “I1” clarity.\n\n\nSolution\n\nThis can be done in a few ways, but Seaborn interfaces with pandas-like dataframes seamlessly to make these simple data-manging tasks easy.\nsns.relplot(x=\"carat\", y=\"price\", hue=\"cut\",size='color',\n        sizes=(10, 200), alpha=.5, palette=\"muted\",\n        height=6, data=data.query('clarity == \"I1\"'))\n\n\n\nCaching results and Building manipulations in memory\nTechniques for writing efficient Python:\n\nUnderstanding the difference between mutable and immutable types\nUse np functions rather than function written in pure Python\nConsider generators\nCache results\n\nA few useful links with ideas on how to do this:\n\nhttps://caam37830.github.io/book/index.html\nhttps://python-course.eu/\nhttps://realpython.com/fibonacci-sequence-python/\n\ndef FibonacciGenerator(n):\n    \"\"\" \n    note: n is limit of fibonacci value rather than count\n    \"\"\"\n    a = 0\n    b = 1\n    while a &lt; n:\n        yield a\n        a, b = b, a + b\n        \n# Recursive slower case\ndef fibonacci_of(n):\n    if n in {0, 1}:  # Base case\n        return n\n    return fibonacci_of(n - 1) + fibonacci_of(n - 2)  #return same function \n\ndef is_even(sequence):\n    \"\"\" reduces a sequence to even numbers\n    \"\"\"\n    for n in sequence:\n        if n % 2 == 0:\n            yield n\n# Can consume generators by converting to list\nlist(is_even([1,2,3,4]))\nlist(FibonacciGenerator(500))\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n# Build up sequences of manipulations in memory with generators \n# and selectively trigger consumption for efficiency\nlist(is_even(FibonacciGenerator(500)))\n[0, 2, 8, 34, 144]\n%%timeit #758 nanos ± 0.393 ns per loop\nresults1 = list(FibonacciGenerator(500)) #generator brought into memory via list\n756 ns ± 5.39 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n%%timeit #192 microsec ± 1.92 ns per loop\nresults2 = [fibonacci_of(n) for n in range(15)]\n189 µs ± 352 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n# Caching is another method to improve efficiency, new syntax as of Python 3.9\nfrom functools import cache\n\n@cache\ndef factorial(n):\n    return n * factorial(n-1) if n else 1\nfactorial(10) # no previously cached result, makes 11 recursive calls\nfactorial(5) # just looks up cached value result\nfactorial(12) # makes two new recursive calls, the other 10 are cached\n479001600\n\n\nKey points\n\npd.DataFrame and pd.Series are the most common data structures for tabular data.\nFunctions are “attatched” to the objects. For example pd.Series.sum(), pd.Series.str.contains(), pd.Series.quantile() etc\nThe Pandas API is an invaluable reference to remember the notations."
  },
  {
    "objectID": "notebooks/test_ML1.html",
    "href": "notebooks/test_ML1.html",
    "title": "Parallel Python with Dask",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define a list of models to train\nmodels = [\n    ('Logistic Regression', LogisticRegression(max_iter=200)),\n    ('K-Nearest Neighbors', KNeighborsClassifier()),\n    ('Random Forest', RandomForestClassifier()),\n    ('Support Vector Machine', SVC())\n]\n\n# Train and evaluate each model\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'{name}: {accuracy:.2f}')\nAttributeError: module 'numpy' has no attribute 'matrix'"
  },
  {
    "objectID": "notebooks/polars-example.html",
    "href": "notebooks/polars-example.html",
    "title": "Polars - Example",
    "section": "",
    "text": "TO DO - real world data??\n\n# Load the diamonds dataset from seaborn\ndiamonds = sns.load_dataset('diamonds')\n\n# Convert the pandas DataFrame to a polars DataFrame\ndiamonds_df = pl.from_pandas(diamonds)\n\nprint(diamonds_df)\nshape: (53_940, 10)\n┌───────┬───────────┬───────┬─────────┬───┬───────┬──────┬──────┬──────┐\n│ carat ┆ cut ┆ color ┆ clarity ┆ … ┆ price ┆ x ┆ y ┆ z │\n│ — ┆ — ┆ — ┆ — ┆ ┆ — ┆ — ┆ — ┆ — │\n│ f64 ┆ cat ┆ cat ┆ cat ┆ ┆ i64 ┆ f64 ┆ f64 ┆ f64 │\n╞═══════╪═══════════╪═══════╪═════════╪═══╪═══════╪══════╪══════╪══════╡\n│ 0.23 ┆ Ideal ┆ E ┆ SI2 ┆ … ┆ 326 ┆ 3.95 ┆ 3.98 ┆ 2.43 │\n│ 0.21 ┆ Premium ┆ E ┆ SI1 ┆ … ┆ 326 ┆ 3.89 ┆ 3.84 ┆ 2.31 │\n│ 0.23 ┆ Good ┆ E ┆ VS1 ┆ … ┆ 327 ┆ 4.05 ┆ 4.07 ┆ 2.31 │\n│ 0.29 ┆ Premium ┆ I ┆ VS2 ┆ … ┆ 334 ┆ 4.2 ┆ 4.23 ┆ 2.63 │\n│ 0.31 ┆ Good ┆ J ┆ SI2 ┆ … ┆ 335 ┆ 4.34 ┆ 4.35 ┆ 2.75 │\n│ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … ┆ … │\n│ 0.72 ┆ Ideal ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.75 ┆ 5.76 ┆ 3.5 │\n│ 0.72 ┆ Good ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.69 ┆ 5.75 ┆ 3.61 │\n│ 0.7 ┆ Very Good ┆ D ┆ SI1 ┆ … ┆ 2757 ┆ 5.66 ┆ 5.68 ┆ 3.56 │\n│ 0.86 ┆ Premium ┆ H ┆ SI2 ┆ … ┆ 2757 ┆ 6.15 ┆ 6.12 ┆ 3.74 │\n│ 0.75 ┆ Ideal ┆ D ┆ SI2 ┆ … ┆ 2757 ┆ 5.83 ┆ 5.87 ┆ 3.64 │\n└───────┴───────────┴───────┴─────────┴───┴───────┴──────┴──────┴──────┘\n# random\nimport polars as pl\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the diamonds dataset from seaborn\ndiamonds = sns.load_dataset('diamonds')\n\n# Convert the pandas DataFrame to a polars DataFrame\ndiamonds_df = pl.from_pandas(diamonds)\n\n# Encode categorical variables\ndiamonds_df = diamonds_df.with_column(pl.col(\"cut\").cast(pl.Categorical))\n\n# Split the data into features and target\nX = diamonds_df.select([\"carat\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"]).to_pandas()\ny = diamonds_df.select([\"cut\"]).to_pandas()\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n   carat        cut color clarity  depth  table  price     x     y     z\n0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43\n1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31\n2 0.23 Good E VS1 56.9 65.0 327 4.05 4.07 2.31\n3 0.29 Premium I VS2 62.4 58.0 334 4.20 4.23 2.63\n4 0.31 Good J SI2 63.3 58.0 335 4.34 4.35 2.75\n… … … … … … … … … … …\n53935 0.72 Ideal D SI1 60.8 57.0 2757 5.75 5.76 3.50\n53936 0.72 Good D SI1 63.1 55.0 2757 5.69 5.75 3.61\n53937 0.70 Very Good D SI1 62.8 60.0 2757 5.66 5.68 3.56\n53938 0.86 Premium H SI2 61.0 58.0 2757 6.15 6.12 3.74\n53939 0.75 Ideal D SI2 62.2 55.0 2757 5.83 5.87 3.64\n[53940 rows x 10 columns]"
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html",
    "href": "notebooks/01d-dask_delayed.html",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "",
    "text": "Can Dask be used for embarassingly parallel problems?\nHow do you apply it to real functions?\nIn this example we will explore the Schrodinger equation, and how we can use dask for an embarassingly parallel problem.\nSee here for similar problems: https://github.com/natsunoyuki/Computational_Physics_in_Python\nDefine a “computationally intensive” function. Here we are solving for the eigenvalues of \\({\\displaystyle i\\hbar {\\frac {d}{dt}}\\vert Ψ (t)\\rangle ={\\hat {H}}\\vert Ψ (t)\\rangle }\\)\nDefine a function to plot H.\nDefine some potenial energy functions we want to explore.\nLet’s get an idea for how long our schrodinger equation takes to solve.\nEnergy eigenvalues:\n1: 1.00\n2: 3.00\n3: 4.99\n4: 6.99\n5: 8.98\nLet’s profile this function. Is there any way we can speed it up? Or apply some of the techniques we have learned? We can use the iPython/Jupyter magic command %%prun which uses cProfile.\nTLDR: maybe not! Not all code can be “dasked” or parallelised easily.\nOkay. There may not be anything we can improve of greatly. The slowest part is a highly optimised scipy subroutine that is calling fortran under-the-hood! So what if we wanted to run this function 2 times, 3 times, a million times? Perhaps trying different configuration parameters, or specifically here, different potential energy functions."
  },
  {
    "objectID": "notebooks/01d-dask_delayed.html#dask-delayed",
    "href": "notebooks/01d-dask_delayed.html#dask-delayed",
    "title": "Scientific Computing - Applications to Quantum",
    "section": "Dask Delayed",
    "text": "Dask Delayed\nNow let’s try and solve the three variations in parallel. This is an embarassingly parallel problem, as each operation is completely seperate from the other.\nimport dask\n%%time\nlazy_H = []\nfor f in [Vfun1,Vfun2,Vfun3]:\n    H_temp = dask.delayed(schrodinger1D)(f)\n    lazy_H.append(H_temp)\nlazy_H\n%%time \nHH = dask.compute(*lazy_H)\nDone! That is it. You can now run the schrodinger1D as many times as you like in parallel and dask will take of distributing out the work to as many cpus as it can gets its threads on!\n\nChallenge 1\nCan you modify some of the parameters in the schrodinger1D function and see how the timing changes?\n\n\nSolution\n\nTry changing the xmin, xmax, and Nx parameter. These adjust the resolution of the model. You can quickly see how you may want to parallelise this code as each numerical solution can take a long time at high-resolutions.\nxmin = -100\nxmax = 100\nNx = 500\nThen re-run with\n%%time\nH = schrodinger1D(Vfun1)\n\n\n\nExercise 1 Multiple inputs\nCan you re-write the the schrodinger1D function to accept “params” as an argument, then run multiple parameter configurations with a single Potential Energy function?\n\n\nStep 1\n\nModify the schrodinger1D function to accept an additional argument, and pass that argument to the Vfun call.\n#Need to change line 1\ndef schrodinger1D(Vfun, params): \n    ...\n    # And change line 29\n    V = Vfun(x, params = params)\n\n\n\nStep 2\n\nChoose the Vfun you want to explore, and make a list of parameters we want to sweep. I will be looking at Vfun3. A way to make a set of params is to use the product function from the itertools package.\nimport itertools\nparam_config = [[-1,0,1],[-1,0,1],[-1,0,1]]\nparams=list(map(list, itertools.product(*param_config)))\nprint(params)\n[-1, -1, -1]\n[-1, -1, 0]\n[-1, -1, 1]\n[-1, 0, -1]\n[-1, 0, 0]\n[-1, 0, 1]\n[-1, 1, -1]\n[-1, 1, 0]\n[-1, 1, 1]\n[0, -1, -1]\n[0, -1, 0]\n[0, -1, 1]\n[0, 0, -1]\n[0, 0, 0]\n[0, 0, 1]\n[0, 1, -1]\n[0, 1, 0]\n[0, 1, 1]\n[1, -1, -1]\n[1, -1, 0]\n[1, -1, 1]\n[1, 0, -1]\n[1, 0, 0]\n[1, 0, 1]\n[1, 1, -1]\n[1, 1, 0]\n[1, 1, 1]\n\n\n\nStep 3\n\nRe-write the dask delayed function to include your new paramaters.\n%%time\nlazy_H = []\nfor param in params:\n    print(params)\n    H_temp = dask.delayed(schrodinger1D)(Vfun3, param)\n    lazy_H.append(H_temp)\n    \nlazy_H.compute()\n    \n\n\n\nExercise 2 Multiprocessing vs Dask\nHow do you implement this same functionality in native Python Multiprocessing?\n\n\nSolution\n\nThe answer looks something like this:\nwith Pool(processes=ncpus) as pool: \n    y=pool.imap(schrodinger1D, [Vfun1,Vfun2,Vfun3])\n    pool.close()\n    pool.join()\n    outputs = [result for result in y]\nSee the complete solution and description here: schrodinger1D.py\n\n\n\nKey points\n\nDask can be used for embarassingly parallel problems.\nFinding where to make your code faster and understanding what kind of code/data you can determine which approaches you use."
  },
  {
    "objectID": "notebooks/01a-fundamentals.html",
    "href": "notebooks/01a-fundamentals.html",
    "title": "Python fundamentals",
    "section": "",
    "text": "Questions\n\nWhat can Python do?\nHow do I do it?\n\n\n\nObjectives\n\nLearn the basic Python commands\n\n\nGenerally, cells like this are what to type into your Python shell/notebook/colab:\n2+4*10\n42\n\nFunctions\nThese are bits of code you want to perhaps use many times, or keep self contained, or refer to at different points. They can take values as input and give values back (or not).\n#Declare the name of the function\ndef add_numbers(x,y):\n    '''adds two numbers\n    usage: myaddition=addnumbers(x,y)\n    returns: z\n    inputs: x,y\n    x and y are two integers\n    z is the summation of x and y\n    '''\n    \n    z=x+y\n    \n    return(z)\nNote the indentation - Python forces your code to be nicely readable by using ‘whitespace’/indentation to signify what chunks of code are related. You will see this more later, but generally you should try and write readable code and follow style standards\nMany functions have a header - formatted as a multiline comment with three '''. This hopefully will tell you about the function.\nAnyway, let’s run our function, now that we have initialised it!\nadd_numbers(1,2)\n3\n\n\nChallenge\nWrite a function to convert map scale. For example, on a 1:25,000 map (good for hiking!) the distance between two points is 15 cm. How far apart are these in real life? (3750 m).\n[Reminder: 15 cm * 25000 = 375000 cm = 3750 m]\nYour function should take as input two numbers: the distance on the map (in cm) and the second number of the scale and, i.e. calculate_distance(15, 25000) should return 375000\n\n\nSolution\n\n#Declare the name of the function\ndef calculate_distance(distance_cm,scale):\n    '''calculates distance based on map and scale\n    returns: z\n    inputs: distance_cm,scale\n    distance_cm and scale are two integers\n    returns the product of distance_cm and scale\n    '''  \n    \n    return(distance_cm * scale)\n\n\nLet’s quickly make a figure using some sample data.\n#First we have to load some modules to do the work for us.\n#Modules are packages people have written so we do not have to re-invent everything!\n\n#The first is NUMerical PYthon. A very popular matrix, math, array and data manipulation library.\nimport numpy as np\n\n#This is a library for making figures (originally based off Matlab plotting routines)\n#We use the alias 'plt' because we don't want to type out the whole name every time we reference it!\nimport matplotlib.pyplot as plt \n\n# random code from matplotlib docs\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\npeople = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\ny_pos = np.arange(len(people))\nperformance = 3 + 10 * np.random.rand(len(people))\nerror = np.random.rand(len(people))\n\nax.barh(y_pos, performance, xerr=error, align='center')\nax.set_yticks(y_pos, labels=people)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Performance')\nax.set_title('How fast do you want to go today?')\n\nplt.show()\n\n\nKey points\n\nYou can store things in Python in variables\nLists can be used to store objects of different types\nLoops with for can be used to iterate over each object in a list\nFunctions are used to write (and debug) repetitive code once\nPython uses 0-based indexing\nNumpy and matplotlib are two key python libraries for numerical computations and data visualisation, respectively"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel Python",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn intermediate python skills applied to scientific computing and data science."
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Parallel Python",
    "section": "Trainers",
    "text": "Trainers\n\nKristian Maras (Kris) (MSc Mathematics / Ba Commerce)\nThomas Mauch (Thomas) (PhD in astronomy)\nNathaniel (Nate) Butterworth (PhD Computational Geophysics)"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Parallel Python",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nIntroductory Python experience recommended."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Parallel Python",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here."
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Parallel Python",
    "section": "General session timings",
    "text": "General session timings\n\nA. Intoduction and Revise Python Data Manipulation and Pandas Data Structure\nB. Why Polars is a better option for dataframes\nC. Why Dask provides an ecosystem of tools that can run on clusters of machines."
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Parallel Python",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nconda env create -f environment.yml\nconda activate parallel"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Go to Dask Binder\nLaunch the Binder instance at the Dask Example Page and go from there!\nDone. If you would prefer to have a local version of the Python environment we are using, see below for setup instructions.\n\n\nAt home setup\nTo complete the exercises presented in the workshop, you may create a Python environment with the following packages:\nconda create -n quantum python=3.9 numpy scipy pandas scikit-learn seaborn dask jupyterlab -c conda-forge\nAt the time of this workshop, the major package versions were:\ndask=2022.10.0\njupyterlab=3.4.8\nmatplotlib-base=3.5.3\nnumpy=1.23.4\npandas=1.5.1\npython=3.9.13\nscikit-learn=1.1.2\nscipy=1.9.3\nseaborn=0.12.1\nOther combinations may also work."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html",
    "href": "notebooks/01b-TransitionToDask.html",
    "title": "What is Dask?",
    "section": "",
    "text": "Dask\n10 minute intro\nAPI Reference\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\nInternally, a Dask DataFrame is split into many partitions, where each partition is one Pandas DataFrame. When our index is sorted and we know the values of the divisions of our partitions, then we can be clever and efficient with expensive algorithms (e.g. groupby’s, joins, etc…).\nUse Cases:\nDask DataFrame is used in situations where pandas is commonly needed, usually when pandas fails due to data size or speed of computation. Common use cases are:\nDask DataFrame may not be the best choice in the following situations:"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "title": "What is Dask?",
    "section": "Transitioning to Dask DataFrames",
    "text": "Transitioning to Dask DataFrames\n# load ddf from existing df\nddf = dd.from_pandas(df,npartitions = 2) \n# many loading options available\n\nddf #dask dataframe \n# by default it has lazy execution where computation are triggered by compute() (or head) \nddf.compute() # convert dd to pd.DataFrame\nddf.head(2)\n\n# Attributes of Dask dataframe distinct from pd.Dataframe\nddf.npartitions # number of partitions\nddf.divisions # Divisions includes the minimum value of every partition’s index and the maximum value of the last partition’s index\nddf.partitions[1] # access a particular partition\nddf.partitions[1].index # which have similar pd.DataFrame attributes\n\n# Special consideration\n\n# By default, groupby methods return an object with only 1 partition. \n# This is to optimize performance, and assumes the groupby reduction returns an object that is small enough to fit into memory. \n# If your returned object is larger than this, you can increase the number of output partitions using the split_out argument.\nddf.groupby('cut').mean() #npartitions=1\nddf.groupby('cut').mean(split_out=2) #npartitions=2\n/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/dask/dataframe/groupby.py:1351: FutureWarning: In the future, `sort` for groupby operations will default to `True` to match the behavior of pandas. However, `sort=True` does not work with `split_out&gt;1`. To retain the current behavior for multiple output partitions, set `sort=False`.\n  warnings.warn(SORT_SPLIT_OUT_WARNING, FutureWarning)\n/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/dask/dataframe/groupby.py:1351: FutureWarning: In the future, `sort` for groupby operations will default to `True` to match the behavior of pandas. However, `sort=True` does not work with `split_out&gt;1`. To retain the current behavior for multiple output partitions, set `sort=False`.\n  warnings.warn(SORT_SPLIT_OUT_WARNING, FutureWarning)\nDask DataFrame Structure:\n\n\n\n\n\n\n\ncarat\ndepth\ntable\nprice\nx\ny\nz\n\n\nnpartitions=2\n\n\n\n\n\n\n\n\n\n\n\n\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\n\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: truediv, 19 graph layers\n# Dask syntax intentionally mimics most well knows pandas apis\nddf.loc[15:20] # subset rows\nddf[[\"carat\",\"price\"]] # subset columns\nddf.dtypes  # access attributes\nddf.head(3)\nddf.query('price &gt; 50') # same as pd.DataFrame\n\nlazy_manipulations = (ddf.query('price &gt; 50').\n    groupby('clarity').\n    price.mean())\nlazy_manipulations.compute() # trigger computation to pd.DataFrame\n\n# dask aggregate has more features than pandas agg equivalent, supports reductions on the same group.\n\nddf_aggs = (ddf.groupby('cut')\n    .aggregate({\"price\":\"mean\",\"carat\":\"sum\"}))\n\n# Can persist data into RAM if possible making future operations on it faster\nddf_aggs = ddf_aggs.repartition(npartitions = 1).persist()\n\ndf_merged = ddf.merge(ddf_aggs,left_on= \"cut\",right_index=True, suffixes=(\"_original\", \"_aggregated\"))\n\ndf_merged.head(2)\n\n\n\n\n\n\n\n\ncarat_original\ncut\ncolor\nclarity\ndepth\ntable\nprice_original\nx\ny\nz\nprice_aggregated\ncarat_aggregated\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n3457.54197\n15146.84\n\n\n11\n0.23\nIdeal\nJ\nVS1\n62.8\n56.0\n340\n3.93\n3.90\n2.46\n3457.54197\n15146.84\n\n\n\n\n\nNote that not all apis from pandas are available in Dask. For example, ddf.filter(['carat','price']) is not available. For more details and a list of available options, see here.\n\nChallenge\n\nWhat is the price per carat over the entire dataset?\nCreate a column called price_to_carat that calculates this for each row\nCreate a column called expensive that flags whether price is greater than price_to_carat\nHow many expensive diamonds are there\n\n\n\nSolution\n\n\nAverage price to carat $4928\n15003 expensive diamonds compared to whole dataset\n\nprice_per_carat = (ddf.price.sum() / ddf.carat.sum()).compute()\n\nddf = ddf.assign(price_to_carat = ddf.price / ddf.carat)\n\ndef greater_than_avg(price):\n    if price &gt; price_per_carat:\n        return True\n    else:\n        return False\n\nddf = ddf.assign(expensive = ddf.price.apply(greater_than_avg))\nddf.sort_values('expensive',ascending= False).compute()\nnumber_expensive = ddf.expensive.sum().compute()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "href": "notebooks/01b-TransitionToDask.html#dask-best-practice-guide",
    "title": "What is Dask?",
    "section": "Dask Best Practice Guide",
    "text": "Dask Best Practice Guide\n\nUse set_index() sparingly to speed up data naturally sorted on a single index\n\nUse ddf.set_index('column')\n\nPersist intelligently\n\n\nIf you have the available RAM for your dataset then you can persist data in memory. On distributed systems, it is a way of telling the cluster that it should start executing the computations that you have defined so far, and that it should try to keep those results in memory.\n\ndf = df.persist()\n\n\n\nRepartition to reduce overhead\n\nAs you reduce or increase the size of your pandas DataFrames by filtering or joining, it may be wise to reconsider how many partitions you need. Adjust partitions accordingly using repartition.\ndf = df.repartition(npartitions=df.npartitions // 100)\n\nConsider storing large data in Apache Parquet Format (binary column based format)\n\n# Time series data with every second observations from year 2000\nts_data \n\n# dask can use datetime index to reduce data efficiently\nts_data[[\"x\", \"y\"]].resample(\"1h\").mean().head()\n\n# Build up lazy data manipulations and compute selectively to reduce data\n\nts_subset = ts_data.groupby('name').aggregate({\"x\": \"sum\", \"y\": \"max\"})\n\n# Repartition appropriately, smaller dataset doesn't need many partitions\nts_subset = ts_subset.repartition(npartitions= 1)\n\nts_subset.head(10)\n\n# Set index selectively as its expensive\nts_subset = ts_subset.set_index(\"name\")\n\n# Persist in RAM if possible after expensive calculations to rather than continue building lazy operations. \nts_subset = ts_subset.persist()\n\n# Continue with pandas if memory is fine\nts_subset_df = ts_subset.compute()\nts_subset_df.sort_values(\"name\").head(3)\n/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/site-packages/dask/dataframe/core.py:4948: UserWarning: New index has same name as existing, this is a no-op.\n  warnings.warn(\n\n\n\n\n\n\n\nx\ny\n\n\nname\n\n\n\n\n\n\nAlice\n322.982262\n0.999975\n\n\nBob\n34.466002\n0.999986\n\n\nCharlie\n222.431201\n0.999984"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#using-external-functions-in-dask",
    "href": "notebooks/01b-TransitionToDask.html#using-external-functions-in-dask",
    "title": "What is Dask?",
    "section": "Using external functions in Dask",
    "text": "Using external functions in Dask\nfrom sklearn.linear_model import LinearRegression\n\ndef train(partition):\n    if not len(partition):\n        return\n    est = LinearRegression()\n    est.fit(partition[[\"x\"]].values, partition.y.values)\n    return est\n\n'''\nThe meta argument tells Dask how to create the DataFrame or Series that will hold the result of .apply(). \nIn this case, train() returns a single value, so .apply() will create a Series. \nThis means we need to tell Dask what the type of that single column should be and optionally give it a name.\n'''\nresults = ts_subset.groupby(\"name\").apply(\n    train, meta=(\"LinearRegression\", object)\n).compute()\n\nresults[\"Bob\"] # linear model of a particular group\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dataframes-reading-in-messy-data",
    "href": "notebooks/01b-TransitionToDask.html#dataframes-reading-in-messy-data",
    "title": "What is Dask?",
    "section": "DataFrames: Reading in messy data",
    "text": "DataFrames: Reading in messy data\nGo through existing Binder - demostrates both dask and using delayed functions."
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "href": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "title": "What is Dask?",
    "section": "Dask Arrays",
    "text": "Dask Arrays\nimport dask.array as da\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nx\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n762.94 MiB\n7.63 MiB\n\n\nShape\n(10000, 10000)\n(1000, 1000)\n\n\nCount\n1 Graph Layer\n100 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n# numpy syntax as usual\ny = x + x.T\nz = y[::2, 5000:].mean(axis=1) # axis 0 is index, axis 1 is columns\nz\n# Trigger compute and investigate Client\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n39.06 kiB\n3.91 kiB\n\n\nShape\n(5000,)\n(500,)\n\n\nCount\n7 Graph Layers\n10 Chunks\n\n\nType\nfloat64\nnumpy.ndarray\n\n\n\n\n\n\n\n\nMore more info on arrays - Go through tutorial on\nhttps://tutorial.dask.org/"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "href": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "title": "What is Dask?",
    "section": "Diagnostics - Profile resource efficiency in real time",
    "text": "Diagnostics - Profile resource efficiency in real time\nThe Dask Dashboard enables resource monitoring across RAM, CPU, workers, threads and tasks (functions).\nhttps://docs.dask.org/en/stable/dashboard.html\n\nA few key definitions:\n\nBytes Stored and Bytes per Worker: Cluster memory and Memory per worker.\nTask Processing/CPU Utilization/Occupancy: Tasks being processed by each worker/ CPU Utilization per worker/ Expected runtime for all tasks currently on a worker.\nProgress: Progress of a set of tasks.\n\nThere are three different colors of workers in a task graph:\n\nBlue: Processing tasks.\nGreen: Saturated: It has enough work to stay busy.\nRed: Idle: Does not have enough work to stay busy.\nTask Stream: Individual task across threads.\n\nWhite colour represents deadtime.\n\n\n# To load diagnostic in web browser on local\nfrom dask.distributed import Client\nclient = Client()\nclient #client.shutdown after use\n\n     \n    \n        Client\n        Client-c7757bc6-5514-11ed-ba5c-fe453513c75b\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\n28e449c4\n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 5\n\n\nTotal threads: 10\nTotal memory: 32.00 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-00cc99f1-ae30-4653-a674-8f38139fd85b\n\n\n\nComm: tcp://127.0.0.1:63334\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 10\n\n\nStarted: Just now\nTotal memory: 32.00 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: tcp://127.0.0.1:63357\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:63364/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:63337\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-0x5qvryt\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: tcp://127.0.0.1:63355\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:63362/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:63340\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-0s__jczq\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: tcp://127.0.0.1:63358\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:63360/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:63338\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-mnj91k25\n\n\n\n\n\n\n\n\n\n\nWorker: 3\n\n\n\nComm: tcp://127.0.0.1:63356\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:63361/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:63341\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-nvggbtok\n\n\n\n\n\n\n\n\n\n\nWorker: 4\n\n\n\nComm: tcp://127.0.0.1:63359\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:63363/status\nMemory: 6.40 GiB\n\n\nNanny: tcp://127.0.0.1:63339\n\n\n\nLocal directory: /var/folders/1b/_jymrbj17cz6t7cxdl86xshh0000gr/T/dask-worker-space/worker-sgo11i0u"
  },
  {
    "objectID": "notebooks/polars-intro.html",
    "href": "notebooks/polars-intro.html",
    "title": "Polars - Getting to know the Syntax",
    "section": "",
    "text": "Polars is a lightning fast DataFrame library. The key features of polars are:\nFast and Accessible: Written from scratch in Rust, designed close to the machine and without external dependencies. It also has python and R bindings!\nI/O: First class support for all common data storage layers: local, cloud storage & databases.\nHandle Datasets larger than RAM\nIntuitive API: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.\nOut of Core: The streaming API allows you to process your results without requiring all your data to be in memory at the same time\nParallel: Utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration.\nThe philosophy of Polars is to provide a dataframe library that utilises available cores, has an intuitive api and is performant - hence adheres to a strict schema (data-types should be known before running the query)."
  },
  {
    "objectID": "notebooks/polars-intro.html#learning-the-syntax",
    "href": "notebooks/polars-intro.html#learning-the-syntax",
    "title": "Polars - Getting to know the Syntax",
    "section": "Learning the Syntax",
    "text": "Learning the Syntax\n# selecting columns ----------------------------------------------------------------\ndf.select(pl.col(\"float\")) # selecting a column\n\ndf.select(pl.col(\"date\",\"string\")) # selecting multiple columns \n\ndf.select(pl.col(\"*\").exclude(\"string\")) # select all columns then exclude\n\ndf.select(pl.col(\"^(da|fl).*$\")) # supports regular expressions\n\nimport polars.selectors as cs   # selectors are helper funtions for selecting columns\n\ndf.select(cs.integer() | cs.contains(\"ate\")) # select all columns that are integers or contains ate\n\n# filtering rows ---------------------------------------------------------------------\ndf.filter(pl.col(\"integer\") &gt;= 2)  #filtering rows\n\ndf.filter((pl.col(\"integer\") &gt;=2) & \n          (pl.col(\"float\") == 5.0)) #filtering rows with multiple conditions (| = or & = and)\n\n#creating / manipulating columns -----------------------------------------------------\ndf.with_columns((pl.col(\"integer\") + 3).alias(\"new_column\")) # creating column and naming it\n\n#group by aggregations --------------------------------------------------------\ndf.group_by(\"string\").agg(pl.col(\"integer\").sum().alias(\"sum\"),\n                          pl.col(\"date\").sort().first().alias(\"earliest\"), \n                          pl.col(\"float\") / pl.col(\"integer\")) \n                                \n\nshape: (3, 4)\n\n\n\nstring\nsum\nearliest\nfloat\n\n\nstr\ni64\ndatetime[μs]\nlist[f64]\n\n\n\n\n\"a\"\n1\n2025-01-01 00:00:00\n[4.0]\n\n\n\"b\"\n6\n2025-01-02 00:00:00\n[2.5, 3.0]\n\n\n\"c\"\n3\n2025-01-03 00:00:00\n[2.0]\n\n\n\n\n# Can combine expressions for compactness\n\ndf3 = df.with_columns((pl.col(\"float\") * pl.col(\"integer\"))\n                .alias(\"product\")).select(pl.all().exclude(\"integer\"))\n\nFor more information here are some links to relevant documentation:\nSelecting columns\nAggregation\nData Transformation such as join, Concatenation, pivot and unpivot.\nFunctions for expressions.\nCheck out the API documentation for more information\n\n\nData types\nUnder the hood polars use Arrow data types and memory arrays and offer support for String, Numberic, Nested, Temporay and other types. Most data types are specified by the arrow syntax with the exception of String, Categorical and Object types.\nCategorical data represents string data where the values in the column have a finite set of values (yet for performance implementation different to strings). Polars supports both Enum data type, where categories are known up front, and the more flexible Categorical data type where values are not known beforehand. Conversion between them is trivial. Relying on polars inferring the categories with Categorical types comes at a performance cost (as the encoding is a dictionary like object). See Categorical page for more information.\n# Use Enum where categories are known\ncat_types = pl.Enum([\"polar\",\"panda\",\"teddy\"])\nanimals = pl.Series([\"polar\",\"polar\",\"teddy\",\"panda\"],dtype= cat_types)\n# Use Categprical otherwise\nfictional_animals = pl.Series([\"poobear\",\"minimouse\",\"teddy\",\"poobear\"],dtype= pl.Categorical)"
  },
  {
    "objectID": "notebooks/polars-intro.html#lazy-eager-api",
    "href": "notebooks/polars-intro.html#lazy-eager-api",
    "title": "Polars - Getting to know the Syntax",
    "section": "Lazy / Eager API",
    "text": "Lazy / Eager API\nPolars supports two modes of operation: lazy and eager. In the eager API the query is executed immediately while in the lazy API the query is only evaluated once it is ‘needed’. Deferring the execution to the last minute can have significant performance advantages.\nBefore we explore the two run this code to fetch the iris dataset\n# fetch iris dataset and save as csv in local path\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\niris = fetch_ucirepo(id=53) \nX = iris.data.features \ny = iris.data.targets \niris_df = pd.concat([X, y], axis=1)\niris_df.rename(columns={'class': 'species'}, inplace=True)\niris_df.to_csv(\"iris_data.csv\", index=False)\nIn this example we use the eager API we\n1. Read the iris dataset.\n2. Filter the dataset based on sepal length\n3. Calculate the mean of the sepal width per species\nEvery step is executed immediately returning the intermediate results. This can be very wasteful as we might do work or load extra data that is not being used.\ndf = pl.read_csv(\"iris_data.csv\")\ndf_small = df.filter(pl.col(\"sepal length\") &gt; 5)\ndf_agg = df_small.group_by(\"species\").agg(pl.col(\"sepal width\").mean())\nIf we instead used the lazy API and waited on execution until all the steps are defined then the query planner could perform various optimizations. In this case:\n\nPredicate pushdown: Apply filters as early as possible while reading the dataset, thus only reading rows with sepal length greater than 5.\nProjection pushdown: Select only the columns that are needed while reading the dataset, thus removing the need to load additional columns (e.g. petal length & petal width)\n\nq = (\n    pl.scan_csv(\"iris_data.csv\") #doesnt read it all before other operation is performed\n        .filter(pl.col(\"sepal length\") &gt; 5)\n        .group_by(\"species\").agg(pl.col(\"sepal width\").mean())\n)\n\nq # a lazyframe\n\ndf_agg = q.collect() # inform polars that you want to execute the query\nOne additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once Polars can execute the query in batches allowing you to process datasets that are larger-than-memory. To tell Polars we want to execute a query in streaming mode we pass the streaming=True argument to collect.\nWhen to use Lazy versus Eager:\nIn general the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don’t know yet what your query is going to look like\n\nTypical pipeline Demo\nGiven your new knowledge of polars, here is an example on how to integrate into the usual pipeline consisting of:\n\nData ingestion and manipulation.\nData visualization\nModel Preperation - Training and Testing\nPrediction\n\n\n# Do you aggregating and manipulation to clean data\n# keep \"as lazy\" as possible\nq = (\n    pl.scan_csv(\"iris_data.csv\") # data manipulation stuff - trivial example\n        .filter(pl.col(\"sepal length\") &gt; 2)\n)\n\niris_df = q.collect() #trigger computation on and store the polars dataframe"
  },
  {
    "objectID": "notebooks/polars-intro.html#ecosystem",
    "href": "notebooks/polars-intro.html#ecosystem",
    "title": "Polars - Getting to know the Syntax",
    "section": "Ecosystem",
    "text": "Ecosystem\nOn this page you can find a non-exhaustive list of libraries and tools that support Polars. As the data ecosystem is evolving fast, more libraries will likely support Polars in the future. One of the main drivers is that Polars makes adheres its memory layout to the Apache Arrow spec.\n# Polars dataframs has kMachine learning \n\n# Example preprocessing with Polars: Adding a new feature\niris_df = df.with_columns(\n    (df['sepal length'] * df['sepal width']).alias('sepal area')\n)\n\n# Separate features and target for scikit-learn\nX = iris_df.select(['sepal length', 'sepal width', 'petal length', 'petal width', 'sepal area'])\ny = iris_df.select('species').to_series()"
  },
  {
    "objectID": "notebooks/01b-dask_demo.html",
    "href": "notebooks/01b-dask_demo.html",
    "title": "Dask fundamentals",
    "section": "",
    "text": "Questions\n\nExploring Dask in Data bound and Input-Output bound computations\n\n\n\nObjectives\n\nTODO\n\n\n#More\nTODO  - more stuff\nConcept\n# Code\n\nChallenge\nTO DO - This describes a challenge.\n\n\nSolution\n\n#Declare the name of the function\ndef solveme(x):\n    '''Run code\n    '''  \n    \n    pass\n\n\n\nKey points\n\nTODO1\nTODO1"
  },
  {
    "objectID": "notebooks/01c-scientific_computing.html",
    "href": "notebooks/01c-scientific_computing.html",
    "title": "Background: What is Parallel Python?",
    "section": "",
    "text": "Parallel computing is when many different tasks are carried out simultaneously. Python does this by creating independent processes that ship data, program files and libraries to an isolated ecosystem where computation is performed. There are three main models for parallel computing:"
  },
  {
    "objectID": "notebooks/01c-scientific_computing.html#some-terminology-processes-threads-and-shared-memory",
    "href": "notebooks/01c-scientific_computing.html#some-terminology-processes-threads-and-shared-memory",
    "title": "Background: What is Parallel Python?",
    "section": "Some terminology: Processes, threads and shared memory",
    "text": "Some terminology: Processes, threads and shared memory\n\nA process is a collection of resources including program files and memory that operates as an independent entity. Since each process has a seperate memory space, it can operate independently from other processes. It cannot easily access shared data in other processes.\nA thread is the unit of execution within a process. A process can have anywhere from just one thread to many threads. Threads are considered lightweight because they use far less resources than processes. Threads also share the same memory space so are not independent.\n\n\n\n\n\n\n\n\n\nThe designers of the Python language made the choice that only one thread in a process can run actual Python code by using the so-called global interpreter lock (GIL).\nExternal libraries (NumPy, SciPy, Pandas, etc), written in C or other languages, can release the lock and run multi-threaded. Code writen in native Python has the GIL limitation.\nThe multiprocessing library can be used to release the GIL on native Python code.\nimport dask\nimport dask.distributed as dd\nimport dask.array as da\nimport dask.dataframe as dd\nimport pandas as pd\nimport random\nfrom multiprocessing import Pool ### The default pool makes one process per CPU\n\nSpeed up a function that could take a while to run\n\n# reference:\n# https://aaltoscicomp.github.io/python-for-scicomp/parallel/\n\ndef sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n# Using apply from pandas\nps = pd.Series([10**5,20**5])\nps.apply(sample)\n0    3.13884\n1    3.14272\ndtype: float64\n# Create a pool object from with a with statement \nwith Pool() as p:\n    result = p.map(sample,ps)\n    # will engage p.close() automatically\nProcess SpawnPoolWorker-1:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-2:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-10:\nProcess SpawnPoolWorker-8:\nProcess SpawnPoolWorker-11:\nProcess SpawnPoolWorker-9:\nProcess SpawnPoolWorker-3:\nProcess SpawnPoolWorker-7:\nProcess SpawnPoolWorker-4:\nProcess SpawnPoolWorker-12:\nProcess SpawnPoolWorker-6:\nProcess SpawnPoolWorker-5:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt: \nMultiprocessing introduces an initial fixed cost in time (creating Pool objects). Knowing what hardware you are working on is needed to tailor the number of processes created with what is available. There is a risk of creating too many processes which make the initial fixed cost excessively large.\n\n\nWe will come back to the below alternative (Dask) afterwards\n# Create the dask equivalent input\nds = dd.from_pandas(ps,npartitions = 2)\n%%timeit #605 ms ± 10.9 ms per loop\nresult = ds.apply(sample,meta=('x', 'float64')).mean().compute()\n894 ms ± 9.68 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n%%timeit # 1.08 s ± 47.8 ms per loop\np = Pool()\nresult = p.map(sample,ps)\np.close()\nProcess SpawnPoolWorker-13:\nProcess SpawnPoolWorker-14:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 367, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'sample' on &lt;module '__main__' (built-in)&gt;\nProcess SpawnPoolWorker-20:\nProcess SpawnPoolWorker-24:\nProcess SpawnPoolWorker-21:\nProcess SpawnPoolWorker-23:\nProcess SpawnPoolWorker-16:\nProcess SpawnPoolWorker-22:\nProcess SpawnPoolWorker-17:\nTraceback (most recent call last):\nProcess SpawnPoolWorker-19:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nProcess SpawnPoolWorker-15:\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\nProcess SpawnPoolWorker-18:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nTraceback (most recent call last):\nKeyboardInterrupt\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n    res = self._reader.recv_bytes()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 221, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 419, in _recv_bytes\n    buf = self._recv(4)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/connection.py\", line 384, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n    task = get()\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n    with self._rlock:\n  File \"/Users/darya/opt/miniconda3/envs/quantum/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt: \n\n\nCompare Multiprocessing to Dask\nDask uses multiprocessing by default to overcome the GIL. Hence comparing the run time of the multiprocessing library to Dask with a function-bound problem will yield similar results.\nYet dask offers an ecosystem of resource management (Scheduler, diagnostics, data partitions and Task Graphs) that make it a more attractive way to achieve the same thing in most cases. Resource management is handled automatically by the scheduler.\n# for reference, delaying the same function.\n@dask.delayed\ndef dd_sample(n):\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n_inside_circle / n * 4\n\n\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\nresult\n3.1386374999999997\n%%timeit #595 ms ± 1.54 ms per loop\nresult = ds.apply(dd_sample,meta=('x', 'float64')).mean().compute()\n855 ms ± 7.66 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nKey points\n\nUsing Multiprocessing (or mpi4py - not covered here) are the traditional ways to make functions run in parallel in Python\nUsing Dask and its ecosystem is the modern approach"
  }
]