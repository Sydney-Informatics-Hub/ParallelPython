[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Parallel Python",
    "section": "",
    "text": "This course is aimed at researchers, students, and industry professionals who want to learn intermediate python skills applied to scientific computing and data science.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Parallel Python",
    "section": "Trainers",
    "text": "Trainers\n\nKristian Maras (Kris) (MSc Mathematics / Ba Commerce)\nNathaniel (Nate) Butterworth (PhD Computational Geophysics), nathaniel.butterworth@sydney.edu.au\nDarya Vanichkina (PhD Bioinformatics, SFHEA)\nTim White (PhD Physics and Astronomy)"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Parallel Python",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nIntroductory Python experience recommended."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Parallel Python",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available here."
  },
  {
    "objectID": "index.html#general-session-timings",
    "href": "index.html#general-session-timings",
    "title": "Parallel Python",
    "section": "General session timings",
    "text": "General session timings\n\nA. Intoduction and Revise Python Data Manipulation and Pandas Data Structure\nB. Dask Fundamentals and application to solving data and computationally bounded bottlenecks.\nC. Scientific Computing Demonstration"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Parallel Python",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nAll content in this course is designed to run on the existing Dask Binder :\nDask Binder\nHence no installation steps are needed prior to the course."
  },
  {
    "objectID": "notebooks/01a-fundamentals.html",
    "href": "notebooks/01a-fundamentals.html",
    "title": "Parallel Python",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01a-pandas_fundamentals.html",
    "href": "notebooks/01a-pandas_fundamentals.html",
    "title": "Parallel Python",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b-dask_demo.html",
    "href": "notebooks/01b-dask_demo.html",
    "title": "Parallel Python",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html",
    "href": "notebooks/01b-TransitionToDask.html",
    "title": "Parallel Python",
    "section": "",
    "text": "Dask\n10 minute intro\nAPI Reference\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\nInternally, a Dask DataFrame is split into many partitions, where each partition is one Pandas DataFrame. When our index is sorted and we know the values of the divisions of our partitions, then we can be clever and efficient with expensive algorithms (e.g. groupby’s, joins, etc…).\nUse Cases:\nDask DataFrame is used in situations where pandas is commonly needed, usually when pandas fails due to data size or computation speed. Common use cases are:\n\nManipulating large datasets, even when those datasets don’t fit in memory\nAccelerating long computations by using many cores\nDistributed computing on large datasets with standard pandas operations like groupby, join, and time series computations\n\nDask DataFrame may not be the best choice in the following situations If :\n\nyour dataset fits comfortably into RAM on your laptop, then you may be better off just using pandas. There may be simpler ways to improve performance than through parallelism\nyour dataset doesn’t fit neatly into the pandas tabular model, then you might find more use in dask.bag or dask.array\nyou need functions that are not implemented in Dask DataFrame, then you might want to look at dask.delayed which offers more flexibility\nyou need a proper database with all that databases offer you might prefer something like Postgres\n\n\n#Import libraries and datasets\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport seaborn as sns\nimport dask.datasets\nimport dask.dataframe as dd\n\nts_data = dask.datasets.timeseries()\ndf = sns.load_dataset('diamonds')\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "href": "notebooks/01b-TransitionToDask.html#transitioning-to-dask-dataframes",
    "title": "Parallel Python",
    "section": "Transitioning to Dask DataFrames",
    "text": "Transitioning to Dask DataFrames\n\n#load ddf from existing df\nddf = dd.from_pandas(df,npartitions = 2) \n#many loading options available\n\nddf #dask dataframe \n#by default it has lazy execution where computation are triggered by compute() (or head) \nddf.compute() #converd dd to pd.DataFrame\nddf.head(2)\n\n#Attributes of Dask dataframe distinct from pd.Dataframe\nddf.npartitions # number of partitians\nddf.divisions #Divisions includes the minimum value of every partition’s index and the maximum value of the last partition’s index\nddf.partitions[1] #access a particular partitian\nddf.partitions[1].index # which have similar pd.DataFrame attributes\n\n#Special consideration\n\n# By default, groupby methods return an object with only 1 partition. \n# This is to optimize performance, and assumes the groupby reduction returns an object that is small enough to fit into memory. \n# If your returned object is larger than this, you can increase the number of output partitions using the split_out argument.\nddf.groupby('cut').mean() #npartitains=1\nddf.groupby('cut').mean(split_out=2) #npartitains=2\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      carat\n      depth\n      table\n      price\n      x\n      y\n      z\n    \n    \n      npartitions=2\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      \n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n    \n    \n      \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: truediv, 19 graph layers\n\n\n\n# Dask syntax intentionally mimics most well knows pandas apis\nddf.loc[15:20] # subset rows\nddf[[\"carat\",\"price\"]] #subset columns\nddf.dtypes  #access attributes\nddf.head(3)\nddf.query('price > 50') #same as pd.DataFrame\n\nlazy_manipulations = (ddf.query('price > 50').\n    groupby('clarity').\n    price.mean())\nlazy_manipulations.compute() #trigger computation to pd.DataFrame\n\n#dask aggregate has more features than pandas agg equivalent, supports reductions on the same group.\n\nddf_aggs = (ddf.groupby('cut')\n    .aggregate({\"price\":\"mean\",\"carat\":\"sum\"}))\n\n#Can persist data into RAM if possible making future operations on it faster\nddf_aggs = ddf_aggs.repartition(npartitions = 1).persist()\n\ndf_merged = ddf.merge(ddf_aggs,left_on= \"cut\",right_index=True, suffixes=(\"_original\", \"_aggregated\"))\n\ndf_merged.head(2)\n#note not all apis from pandas available\n#ddf.filter(['carat','price'])  #not available\n#https://docs.dask.org/en/stable/dataframe-api.html\n\n\n\n\n\n\n  \n    \n      \n      carat_original\n      cut\n      color\n      clarity\n      depth\n      table\n      price_original\n      x\n      y\n      z\n      price_aggregated\n      carat_aggregated\n    \n  \n  \n    \n      0\n      0.23\n      Ideal\n      E\n      SI2\n      61.5\n      55.0\n      326\n      3.95\n      3.98\n      2.43\n      3457.54197\n      15146.84\n    \n    \n      11\n      0.23\n      Ideal\n      J\n      VS1\n      62.8\n      56.0\n      340\n      3.93\n      3.90\n      2.46\n      3457.54197\n      15146.84\n    \n  \n\n\n\n\n\nChallenge\n\nWhat is the price per carat over the entire dataset?\nCreate a column called price_to_carat that calculates this for each row\nCreate a column called expensive that flags whether price is greater than price_to_carat\nHow many expensive diamonds are there\n\n\n\nSolution\n\naverage price to carat $4928 15003 expensive diamonds compared to whole dataset\n\nprice_per_carat = (ddf.price.sum() / ddf.carat.sum()).compute()\n\nddf = ddf.assign(price_to_carat = ddf.price / ddf.carat)\n\ndef greater_than_avg(price):\n    if price > price_per_carat:\n        return True\n    else:\n        return False\n\nddf = ddf.assign(expensive = ddf.price.apply(greater_than_avg))\n\nddf.sort_values('expensive',ascending= False).compute()\n\nnumber_expensive = ddf.expensive.sum().compute()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#demonstrate-best-practice-using-dask",
    "href": "notebooks/01b-TransitionToDask.html#demonstrate-best-practice-using-dask",
    "title": "Parallel Python",
    "section": "Demonstrate Best Practice using Dask",
    "text": "Demonstrate Best Practice using Dask\nBest Practice Guides:\nUse set_index sparingly to speed up data naturally sorted on a single index - Use ddf.set_index(‘column’)\nPersist intelligently: If you have the available RAM for your dataset then you can persist data in memory. On distributed systems, it is a way of telling the cluster that it should start executing the computations that you have defined so far, and that it should try to keep those results in memory. - df = df.persist()\nRepartition to reduce overhead. As you reduce or increase the size of your pandas DataFrames by filtering or joining, it may be wise to reconsider how many partitions you need. Adjust partitians accordingly using repartition. - df = df.repartition(npartitions=df.npartitions // 100)\nConsider storing large data in Apache Parquet Format (binary column based format)\n\n#Time series data with every second observations from year 2000\nts_data \n\n#dask can use datetime index to reduce data efficiently\nts_data[[\"x\", \"y\"]].resample(\"1h\").mean().head()\n\n#Build up lazy data manipulations and compute selectively to reduce data\n\nts_subset = ts_data.groupby('name').aggregate({\"x\": \"sum\", \"y\": \"max\"})\n\n#Repartition appropriately, smaller dataset doesnt need many partitians\nts_subset = ts_subset.repartition(npartitions= 1)\n\nts_subset.head(10)\n\n#Set index selectively as its expensive\nts_subset = ts_subset.set_index(\"name\")\n\n#Persist in RAM if possible after expensive calculations to rather than continue building lazy operations. \nts_subset = ts_subset.persist()\n\n#Continue with pandas if memory is fine\nts_subset_df = ts_subset.compute()\nts_subset_df.sort_values(\"name\").head(3)\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      Alice\n      -76.577676\n      0.999903\n    \n    \n      Bob\n      68.025989\n      0.999999\n    \n    \n      Charlie\n      150.461159\n      0.999970"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#using-external-functions-in-dask",
    "href": "notebooks/01b-TransitionToDask.html#using-external-functions-in-dask",
    "title": "Parallel Python",
    "section": "Using external functions in Dask",
    "text": "Using external functions in Dask\n\nfrom sklearn.linear_model import LinearRegression\n\ndef train(partition):\n    if not len(partition):\n        return\n    est = LinearRegression()\n    est.fit(partition[[\"x\"]].values, partition.y.values)\n    return est\n\n'''\nThe meta argument tells Dask how to create the DataFrame or Series that will hold the result of .apply(). \nIn this case, train() returns a single value, so .apply() will create a Series. \nThis means we need to tell Dask what the type of that single column should be and optionally give it a name.\n'''\nresults = ts_subset.groupby(\"name\").apply(\n    train, meta=(\"LinearRegression\", object)\n).compute()\n\nresults[\"Bob\"] #linear model of a particular group\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dataframes-reading-in-messy-data",
    "href": "notebooks/01b-TransitionToDask.html#dataframes-reading-in-messy-data",
    "title": "Parallel Python",
    "section": "DataFrames: Reading in messy data",
    "text": "DataFrames: Reading in messy data\nGo through existing binder - demostrates both dask and using delayed function"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "href": "notebooks/01b-TransitionToDask.html#dask-arrays",
    "title": "Parallel Python",
    "section": "Dask Arrays",
    "text": "Dask Arrays\n\nimport dask.array as da\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nx\n\n\n\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         762.94 MiB \n                         7.63 MiB \n                    \n                    \n                    \n                         Shape \n                         (10000, 10000) \n                         (1000, 1000) \n                    \n                    \n                         Count \n                         1 Graph Layer \n                         100 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  10000\n  10000\n\n        \n    \n\n\n\n\n#numpy syntax as usual\ny = x + x.T\nz = y[::2, 5000:].mean(axis=1) # axis 0 is index, axis 1 is columns\nz\n#Trigger compute and investigate Client\n\n\n\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         39.06 kiB \n                         3.91 kiB \n                    \n                    \n                    \n                         Shape \n                         (5000,) \n                         (500,) \n                    \n                    \n                         Count \n                         7 Graph Layers \n                         10 Chunks \n                    \n                    \n                     Type \n                     float64 \n                     numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  5000\n  1\n\n        \n    \n\n\n\n\nMore more info on arrays - Go through tutorial on\nhttps://tutorial.dask.org/"
  },
  {
    "objectID": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "href": "notebooks/01b-TransitionToDask.html#diagnostics---profile-resource-efficiency-in-real-time",
    "title": "Parallel Python",
    "section": "Diagnostics - Profile resource efficiency in real time",
    "text": "Diagnostics - Profile resource efficiency in real time\nEnables resource monitoring across RAM, CPU, workers, threads and tasks (functions).\nhttps://docs.dask.org/en/stable/dashboard.html\nDefinitions:\nBytes Stored and Bytes per Worker: Cluster memory and Memory per worker\nTask Processing/CPU Utilization/Occupancy: Tasks being processed by each worker/ CPU Utilization per worker/ Expected runtime for all tasks currently on a worker.\nProgress: Progress of a set of tasks.\nThere are three different colors worker tasks graph:\nBlue Processing tasks.\nGreen Saturated: It has enough work to stay busy.\nRed Idle: Does not have enough work to stay busy.\nTask Stream: Individual task across threads. - White colour represent deadtime.\n\n# To load diagnostic in web browser on local\nfrom dask.distributed import Client\nclient = Client()\nclient #client.shutdown after use\n\n\n\n     \n    \n        Client\n        Client-fddb0738-533b-11ed-8830-b469212762d9\n        \n\n        \n        \n            Connection method: Cluster object\n            Cluster type: distributed.LocalCluster\n        \n        \n\n        \n            \n                \n                    Dashboard:  http://127.0.0.1:8787/status\n                \n                \n            \n        \n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        b7149360\n        \n            \n                \n                    Dashboard: http://127.0.0.1:8787/status\n                \n                \n                    Workers: 4\n                \n            \n            \n                \n                    Total threads: 8\n                \n                \n                    Total memory: 31.86 GiB\n                \n            \n            \n            \n    Status: running\n    Using processes: True\n\n\n            \n        \n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-39946b8c-a667-4260-936e-09e59ef3ac13\n            \n                \n                    \n                        Comm: tcp://127.0.0.1:64856\n                    \n                    \n                        Workers: 4\n                    \n                \n                \n                    \n                        Dashboard: http://127.0.0.1:8787/status\n                    \n                    \n                        Total threads: 8\n                    \n                \n                \n                    \n                        Started: Just now\n                    \n                    \n                        Total memory: 31.86 GiB\n                    \n                \n            \n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n                    \n                        \n                            Comm:  tcp://127.0.0.1:64890\n                        \n                        \n                            Total threads:  2\n                        \n                    \n                    \n                        \n                            Dashboard:  http://127.0.0.1:64891/status\n                        \n                        \n                            Memory:  7.97 GiB\n                        \n                    \n                    \n                        \n                            Nanny:  tcp://127.0.0.1:64860\n                        \n                        \n                    \n                    \n                        \n                            Local directory:  C:\\Users\\nbutter\\AppData\\Local\\Temp\\dask-worker-space\\worker-82k95ncc\n                        \n                    \n\n                    \n\n                    \n\n                \n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 1\n                \n                \n                    \n                        \n                            Comm:  tcp://127.0.0.1:64893\n                        \n                        \n                            Total threads:  2\n                        \n                    \n                    \n                        \n                            Dashboard:  http://127.0.0.1:64895/status\n                        \n                        \n                            Memory:  7.97 GiB\n                        \n                    \n                    \n                        \n                            Nanny:  tcp://127.0.0.1:64861\n                        \n                        \n                    \n                    \n                        \n                            Local directory:  C:\\Users\\nbutter\\AppData\\Local\\Temp\\dask-worker-space\\worker-qjwnr0_y\n                        \n                    \n\n                    \n\n                    \n\n                \n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 2\n                \n                \n                    \n                        \n                            Comm:  tcp://127.0.0.1:64887\n                        \n                        \n                            Total threads:  2\n                        \n                    \n                    \n                        \n                            Dashboard:  http://127.0.0.1:64888/status\n                        \n                        \n                            Memory:  7.97 GiB\n                        \n                    \n                    \n                        \n                            Nanny:  tcp://127.0.0.1:64862\n                        \n                        \n                    \n                    \n                        \n                            Local directory:  C:\\Users\\nbutter\\AppData\\Local\\Temp\\dask-worker-space\\worker-k00spw3h\n                        \n                    \n\n                    \n\n                    \n\n                \n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 3\n                \n                \n                    \n                        \n                            Comm:  tcp://127.0.0.1:64894\n                        \n                        \n                            Total threads:  2\n                        \n                    \n                    \n                        \n                            Dashboard:  http://127.0.0.1:64896/status\n                        \n                        \n                            Memory:  7.97 GiB\n                        \n                    \n                    \n                        \n                            Nanny:  tcp://127.0.0.1:64859\n                        \n                        \n                    \n                    \n                        \n                            Local directory:  C:\\Users\\nbutter\\AppData\\Local\\Temp\\dask-worker-space\\worker-0uhuqhig\n                        \n                    \n\n                    \n\n                    \n\n                \n            \n            \n        \n        \n\n    \n\n\n        \n    \n\n            \n        \n\n    \n\n\n\n\n#Example of efficient resource utilisation\nimport dask.array as da\nx = da.random.random(size = (10_000,10_000,10), chunks= (1000,1000,5))\ny = da.random.random(size = (10_000,10_000,10), chunks= (1000,1000,5))\nz = (da.arcsin(x) + da.arcsin(y)).sum(axis = (1,2))\nz.compute()\n\n\n# Inefficient resource utilisation - dask introduces too much overhead for simple sizes np handles well\nx = da.random.random(size = (10_000_000),chunks = (1000,))\nx.sum().compute()\n\n\nKey points\n\nFamilarity of Dask API to pandas makes the transition easy compared to alternatives - although not all functions are replicated.\nScaling up to distributed systems, or down to simple running on your laptop, makes your code easily transferable between different resources.\nDask enables parallelism without low level alterations in code"
  },
  {
    "objectID": "notebooks/01c-scientific_computing.html",
    "href": "notebooks/01c-scientific_computing.html",
    "title": "Parallel Python",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/test.html",
    "href": "notebooks/test.html",
    "title": "Parallel Python",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "At home setup\nTo complete the exercises presented in the workshop, you may create a Python environment with the following packages:\nconda create -n quantum python=3.9 numpy scipy pandas scikit-learn seaborn dask jupyterlab -c conda-forge\nAt the time of this workshop, the major package versions were:\ndask=2022.10.0\njupyterlab=3.4.8\nmatplotlib-base=3.5.3\nnumpy=1.23.4\npandas=1.5.1\npython=3.9.13\nscikit-learn=1.1.2\nscipy=1.9.3\nseaborn=0.12.1\nOther combinations may also work.\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  }
]